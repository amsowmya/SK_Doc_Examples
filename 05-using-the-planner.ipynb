{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kernel(plugins=KernelPluginCollection(plugins={}), prompt_template_engine=PromptTemplateEngine(), memory=NullMemory(), text_completion_services={'chat_completion': <function Kernel.add_text_completion_service.<locals>.<lambda> at 0x000001F7E94F8F70>}, chat_services={'chat_completion': <function Kernel.add_chat_service.<locals>.<lambda> at 0x000001F7C8E19D30>}, text_embedding_generation_services={}, default_text_completion_service='chat_completion', default_chat_service='chat_completion', default_text_embedding_generation_service=None, retry_mechanism=PassThroughWithoutRetry(), function_invoking_handlers={}, function_invoked_handlers={})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel.add_chat_service(\n",
    "    \"chat_completion\",\n",
    "    AzureChatCompletion(\n",
    "        deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "        endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"\"\"\n",
    "Tommorrow is Valentine's day. I need to come up with a few date ideas. She speaks French so write it in English.\n",
    "Convert the text to uppercase\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing  plugins to the plaanner\n",
    "\n",
    "The planner needs to know what plugins are available to it. Here we'll give it access to the SummarizePlugin and WriterPlugin we have defined on disk. This will include many semantic functions, of which the planner will intelligently choose a subset.\n",
    "\n",
    "You can also include native functions as well. Here we'll add the TextPlugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.core_plugins.text_plugin import TextPlugin\n",
    "\n",
    "plugins_directory = \"samples\\\\plugins\"\n",
    "summarize_plugin = kernel.import_semantic_plugin_from_directory(plugins_directory, \"SummarizePlugin\")\n",
    "writer_plugin = kernel.import_semantic_plugin_from_directory(plugins_directory, \"WriterPlugin\")\n",
    "text_plugin = kernel.import_plugin(TextPlugin(), \"TextPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at a basic planner. The BasicPlanner produces a JSON-based plan that aims to solve the provided ask sequentially and evaluated in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planning.basic_planner import BasicPlanner\n",
    "\n",
    "planner = BasicPlanner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_plan = await planner.create_plan(ask, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"input\": \"Valentine's Day Date Ideas\",\n",
      "    \"subtasks\": [\n",
      "        {\"function\": \"WriterPlugin.Brainstorm\"},\n",
      "        {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\n",
      "        {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"English\"}},\n",
      "        {\"function\": \"TextPlugin.uppercase\"}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(basic_plan.generated_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the Planner took my ask and converted it into an JSON-based plan detailing how the AI would go about solving this task, making use of the plugins that the Kernel has available to it.\n",
    "\n",
    "As you can see in the above plan, the AI has determined which functions to call in order to fulfill the user ask. The output of each step of the plan becomes the input to the next function.\n",
    "\n",
    "Let's also define an inline plugin and have it be available to the Planner. Be sure to give it a function name and plugin name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Variable `$to` not found\n",
      "Variable `$sender` not found\n"
     ]
    }
   ],
   "source": [
    "result = await planner.execute_plan(basic_plan, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAKE SURE YOU ONLY USE ENGLISH.\n",
      "\n",
      "HELLO,\n",
      "\n",
      "I HOPE YOU ARE DOING WELL. I WANTED TO SUGGEST SOME FUN AND ROMANTIC ACTIVITIES THAT WE COULD DO TOGETHER:\n",
      "\n",
      "1. WE COULD HAVE A PICNIC IN THE PARK, SURROUNDED BY NATURE AND FRESH AIR.\n",
      "2. COOKING A ROMANTIC DINNER TOGETHER COULD BE A FUN AND INTIMATE WAY TO SPEND AN EVENING.\n",
      "3. GOING TO A WINE TASTING WOULD BE A GREAT OPPORTUNITY TO TRY NEW WINES AND LEARN MORE ABOUT THEM.\n",
      "4. ICE SKATING IS ALWAYS A FUN AND PLAYFUL ACTIVITY THAT WE COULD ENJOY TOGETHER.\n",
      "5. TAKING A HOT AIR BALLOON RIDE WOULD BE A UNIQUE AND EXCITING EXPERIENCE THAT WE COULD SHARE.\n",
      "6. WE COULD ALSO HAVE A COZY NIGHT IN AND WATCH A ROMANTIC MOVIE TOGETHER.\n",
      "7. GOING ON A HIKE AND HAVING A PICNIC AT THE TOP WOULD BE A GREAT WAY TO ENJOY THE OUTDOORS AND EACH OTHER'S COMPANY.\n",
      "8. TAKING A DANCE CLASS TOGETHER COULD BE A FUN AND ROMANTIC WAY TO LEARN SOMETHING NEW.\n",
      "9. VISITING A LOCAL MUSEUM OR ART GALLERY WOULD BE A GREAT WAY TO APPRECIATE ART AND CULTURE TOGETHER.\n",
      "10. FINALLY, WE COULD HAVE A RELAXING SPA DAY TOGETHER AND ENJOY SOME PAMPERING AND RELAXATION.\n",
      "\n",
      "LET ME KNOW WHAT YOU THINK AND IF YOU HAVE ANY OTHER IDEAS!\n",
      "\n",
      "THANK YOU,\n",
      "[YOUR NAME]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Plan Object Model\n",
    "To build more advanced planners, we need to introduce a proper Plan object that can contain all the necessary state and information needed for high quality plans.\n",
    "\n",
    "To see what that object model is, look at (https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/planning/plan.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sequential Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequentioal planner is an XML-based step-by-step planner.You can see the prompt used for it here\n",
    "\n",
    "(https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/planning/sequential_planner/Plugins/SequentialPlanning/skprompt.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planning.sequential_planner import SequentialPlanner\n",
    "\n",
    "planner = SequentialPlanner(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_plan = await planner.create_plan(goal=ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a string to uppercase. : {'variables': {'input': ''}}\n"
     ]
    }
   ],
   "source": [
    "for step in sequential_plan._steps:\n",
    "    print(step.description, \":\", step._state.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await sequential_plan.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Action Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The action planner takes in a list of functions and the goal, and outputs a single function to use that is appropriate to meet that goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planning import ActionPlanner\n",
    "\n",
    "planner = ActionPlanner(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='text', description=None, functions={'lowercase': KernelFunction(plugin_name='text', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={'SummarizePlugin': KernelPlugin(name='SummarizePlugin', description=None, functions={'MakeAbstractReadable': KernelFunction(plugin_name='SummarizePlugin', description='Given a scientific white paper abstract, rewrite it to make it more readable', name='MakeAbstractReadable', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7C8E19DC0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 2.0}, ai_model_id=None, frequency_penalty=2.0, logit_bias={}, max_tokens=4000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Notegen': KernelFunction(plugin_name='SummarizePlugin', description='Automatically generate compact notes for any text or text document.', name='Notegen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B0D0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4AF0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Summarize': KernelFunction(plugin_name='SummarizePlugin', description='Summarize given text or any text document', name='Summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E8A4FEE0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E8A7BDC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Topics': KernelFunction(plugin_name='SummarizePlugin', description='Analyze given text or document and extract key topics worth remembering', name='Topics', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B280>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B1F0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 128, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=128, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'Acronym': KernelFunction(plugin_name='WriterPlugin', description='Generate an acronym for the given concept or phrase', name='Acronym', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B940>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9445A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymGenerator': KernelFunction(plugin_name='WriterPlugin', description='Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.', name='AcronymGenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B700>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.7, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymReverse': KernelFunction(plugin_name='WriterPlugin', description='Given a single word or acronym, generate the expanded form matching the acronym letters.', name='AcronymReverse', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BA60>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B9D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.8, 'frequency_penalty': 0.0, 'stop_sequences': ['#END#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.8, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Brainstorm': KernelFunction(plugin_name='WriterPlugin', description='Given a goal or topic description generate a list of ideas', name='Brainstorm', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BDC0>, parameters=[ParameterView(name='input', description='A topic description or goal.', default_value='', type_=None, required=False), ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['##END##']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Must: brainstorm ideas and create a list.\\nMust: use a numbered list.\\nMust: only one list.\\nMust: end list with ##END##\\nShould: no more than 10 items.\\nShould: at least 3 items.\\nTopic: Valentine's Day Date Ideas\\nStart.\\n\"}], extra_body=None), chat_prompt_template=None), 'EmailGen': KernelFunction(plugin_name='WriterPlugin', description='Write an email from the given bullet points', name='EmailGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BD30>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BC10>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EmailTo': KernelFunction(plugin_name='WriterPlugin', description='Turn bullet points into an email to someone, using a polite tone', name='EmailTo', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BEE0>, parameters=[ParameterView(name='to', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='sender', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BE50>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Rewrite my bullet points into an email featuring complete sentences. Use a polite and inclusive tone.  \\n\\n[Input]\\nToby,\\n\\n- Macbeth, King Scotland\\n- Married, Wife Lady Macbeth, No Kids\\n- Dog Toby McDuff. Hunter, dead. \\n- Shakespeare play\\n\\nThanks,\\nDexter\\n\\n+++++\\nHi Toby,\\n\\nThe story of Macbeth\\nMy name is Macbeth. I used to be King of Scotland, but I died. My wife's name is Lady Macbeth and we were married for 15 years. We had no children. Our beloved dog Toby McDuff was a famous hunter of rats in the forest.\\nMy story was immortalized by Shakespeare in a play.\\n\\nThanks,\\nDexter\\n\\n+++++\\n[Input]\\n\\n1. Picnic in the park\\n2. Cooking a romantic dinner together\\n3. Going to a wine tasting\\n4. Ice skating\\n5. Taking a hot air balloon ride\\n6. Watching a romantic movie at home\\n7. Going on a hike and having a picnic at the top\\n8. Taking a dance class together\\n9. Visiting a local museum or art gallery\\n10. Having a spa day together\\n##END##\\n\\nThanks,\\n\\n+++++\\n\"}], extra_body=None), chat_prompt_template=None), 'EnglishImprover': KernelFunction(plugin_name='WriterPlugin', description='Translate text to English and improve it', name='EnglishImprover', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BCA0>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BF70>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 3000, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=3000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapter': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel.', name='NovelChapter', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556040>, parameters=[ParameterView(name='input', description='A synopsis of what the chapter should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme or topic of this novel.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The synopsis of the previous chapter.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556160>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.3, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.3, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapterWithNotes': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel using notes about the chapter to write.', name='NovelChapterWithNotes', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95561F0>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme of this novel.', default_value='', type_=None, required=False), ParameterView(name='notes', description='Notes useful to write this chapter.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The previous chapter synopsis.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95560D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 1024, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelOutline': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of chapter synopsis for a novel or novella', name='NovelOutline', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512A60>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='chapterCount', description='The number of chapters to generate.', default_value='', type_=None, required=False), ParameterView(name='endMarker', description='The marker to use to end each chapter.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512EE0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.1, 'top_p': 0.5, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.1, top_p=0.5, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Rewrite': KernelFunction(plugin_name='WriterPlugin', description='Automatically generate compact notes for any text or text document', name='Rewrite', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512B80>, parameters=[ParameterView(name='style', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512DC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'ShortPoem': KernelFunction(plugin_name='WriterPlugin', description='Turn a scenario into a short and entertaining poem.', name='ShortPoem', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512AF0>, parameters=[ParameterView(name='input', description='The scenario to turn into a poem.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95129D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 60, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=60, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'StoryGen': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of synopsis for a novel or novella with sub-chapters', name='StoryGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556430>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556280>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 250, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=250, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'TellMeMore': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='TellMeMore', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95564C0>, parameters=[ParameterView(name='conversationtype', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='focusarea', description='', default_value='', type_='string', required=False), ParameterView(name='previousresults', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556310>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 500, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=500, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Translate': KernelFunction(plugin_name='WriterPlugin', description='Translate the input into a language of your choice', name='Translate', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556670>, parameters=[ParameterView(name='input', description='Text to translate', default_value='', type_=None, required=False), ParameterView(name='language', description='Language to translate to', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95565E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.7, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['[done]']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Translate the input below into English\\n\\nMAKE SURE YOU ONLY USE English.\\n\\nHi there,\\n\\nI hope this email finds you well. I wanted to share some ideas for fun and romantic activities that we could do together:\\n\\n1. We could have a lovely picnic in the park, surrounded by nature and fresh air.\\n2. Cooking a romantic dinner together could be a fun and intimate way to spend an evening.\\n3. Going to a wine tasting would be a great opportunity to try new wines and learn more about them.\\n4. Ice skating is always a fun and playful activity that we could enjoy together.\\n5. Taking a hot air balloon ride would be a unique and exciting experience that we could share.\\n6. We could also have a cozy night in and watch a romantic movie together.\\n7. Going on a hike and having a picnic at the top would be a great way to enjoy the outdoors and each other's company.\\n8. Taking a dance class together could be a fun and romantic way to learn something new.\\n9. Visiting a local museum or art gallery would be a great way to appreciate art and culture together.\\n10. Finally, we could have a relaxing spa day together and enjoy some pampering and relaxation.\\n\\nLet me know what you think and if you have any other ideas!\\n\\nThanks,\\n[Your Name]\\n\\nTranslation:\\n\"}], extra_body=None), chat_prompt_template=None), 'TwoSentenceSummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in two sentences or less', name='TwoSentenceSummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95563A0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556790>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'TextPlugin': KernelPlugin(name='TextPlugin', description=None, functions={'lowercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_tSNsOTIMHWAhndYw': KernelPlugin(name='p_tSNsOTIMHWAhndYw', description=None, functions={'f_QOkQhOcvcIJkhbYg': KernelFunction(plugin_name='p_tSNsOTIMHWAhndYw', description='Generic function, unknown purpose', name='f_QOkQhOcvcIJkhbYg', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E964DF70>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B550>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\\nYou are a planner for the Semantic Kernel.\\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\\nCreate a list of subtasks based off the [GOAL] provided.\\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\\nBase your decisions on which functions to use from the description and the name of the function.\\nSometimes, a function may take arguments. Provide them if necessary.\\nThe plan should be as short as possible.\\nFor example:\\n\\n[AVAILABLE FUNCTIONS]\\nEmailConnector.LookupContactEmail\\ndescription: looks up the a contact and retrieves their email address\\nargs:\\n- name: the name to look up\\n\\nWriterPlugin.EmailTo\\ndescription: email the input text to a recipient\\nargs:\\n- input: the text to email\\n- recipient: the recipient\\'s email address. Multiple addresses may be included if separated by \\';\\'.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\nWriterPlugin.Summarize\\ndescription: summarize input text\\nargs:\\n- input: the text to summarize\\n\\nFunPlugin.Joke\\ndescription: Generate a funny joke\\nargs:\\n- input: the input to generate a joke about\\n\\n[GOAL]\\n\"Tell a joke about cars. Translate it to Spanish\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"cars\",\\n        \"subtasks\": [\\n            {\"function\": \"FunPlugin.Joke\"},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"Spanish\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nWriterPlugin.Brainstorm\\ndescription: Brainstorm ideas\\nargs:\\n- input: the input to brainstorm about\\n\\nEdgarAllenPoePlugin.Poe\\ndescription: Write in the style of author Edgar Allen Poe\\nargs:\\n- input: the input to write about\\n\\nWriterPlugin.EmailTo\\ndescription: Write an email to a recipient\\nargs:\\n- input: the input to write about\\n- recipient: the recipient\\'s email address.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\n[GOAL]\\n\"Tomorrow is Valentine\\'s day. I need to come up with a few date ideas.\\nShe likes Edgar Allen Poe so write using his style.\\nE-mail these ideas to my significant other. Translate it to French.\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"Valentine\\'s Day Date Ideas\",\\n        \"subtasks\": [\\n            {\"function\": \"WriterPlugin.Brainstorm\"},\\n            {\"function\": \"EdgarAllenPoePlugin.Poe\"},\\n            {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"French\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nTextPlugin.lowercase\\ndescription: Convert a string to lowercase.\\nargs:\\n\\nTextPlugin.trim\\ndescription: Trim whitespace from the start and end of a string.\\nargs:\\n\\nTextPlugin.trim_end\\ndescription: Trim whitespace from the end of a string.\\nargs:\\n\\nTextPlugin.trim_start\\ndescription: Trim whitespace from the start of a string.\\nargs:\\n\\nTextPlugin.uppercase\\ndescription: Convert a string to uppercase.\\nargs:\\n\\nSummarizePlugin.MakeAbstractReadable\\ndescription: Given a scientific white paper abstract, rewrite it to make it more readable\\nargs:\\n- input: \\n\\nSummarizePlugin.Notegen\\ndescription: Automatically generate compact notes for any text or text document.\\nargs:\\n- input: \\n\\nSummarizePlugin.Summarize\\ndescription: Summarize given text or any text document\\nargs:\\n- input: Text to summarize\\n\\nSummarizePlugin.Topics\\ndescription: Analyze given text or document and extract key topics worth remembering\\nargs:\\n- input: \\n\\nWriterPlugin.Acronym\\ndescription: Generate an acronym for the given concept or phrase\\nargs:\\n- input: \\n\\nWriterPlugin.AcronymGenerator\\ndescription: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.AcronymReverse\\ndescription: Given a single word or acronym, generate the expanded form matching the acronym letters.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.Brainstorm\\ndescription: Given a goal or topic description generate a list of ideas\\nargs:\\n- input: A topic description or goal.\\n- INPUT: \\n\\nWriterPlugin.EmailGen\\ndescription: Write an email from the given bullet points\\nargs:\\n- input: \\n\\nWriterPlugin.EmailTo\\ndescription: Turn bullet points into an email to someone, using a polite tone\\nargs:\\n- to: \\n- input: \\n- sender: \\n\\nWriterPlugin.EnglishImprover\\ndescription: Translate text to English and improve it\\nargs:\\n- INPUT: \\n\\nWriterPlugin.NovelChapter\\ndescription: Write a chapter of a novel.\\nargs:\\n- input: A synopsis of what the chapter should be about.\\n- theme: The theme or topic of this novel.\\n- previousChapter: The synopsis of the previous chapter.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelChapterWithNotes\\ndescription: Write a chapter of a novel using notes about the chapter to write.\\nargs:\\n- input: What the novel should be about.\\n- theme: The theme of this novel.\\n- notes: Notes useful to write this chapter.\\n- previousChapter: The previous chapter synopsis.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline\\ndescription: Generate a list of chapter synopsis for a novel or novella\\nargs:\\n- input: What the novel should be about.\\n- chapterCount: The number of chapters to generate.\\n- endMarker: The marker to use to end each chapter.\\n\\nWriterPlugin.Rewrite\\ndescription: Automatically generate compact notes for any text or text document\\nargs:\\n- style: \\n- input: \\n\\nWriterPlugin.ShortPoem\\ndescription: Turn a scenario into a short and entertaining poem.\\nargs:\\n- input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen\\ndescription: Generate a list of synopsis for a novel or novella with sub-chapters\\nargs:\\n- input: \\n\\nWriterPlugin.TellMeMore\\ndescription: Summarize given text or any text document\\nargs:\\n- conversationtype: \\n- input: \\n- focusarea: \\n- previousresults: \\n\\nWriterPlugin.Translate\\ndescription: Translate the input into a language of your choice\\nargs:\\n- input: Text to translate\\n- language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary\\ndescription: Summarize given text in two sentences or less\\nargs:\\n- input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\ndescription: Generic function, unknown purpose\\nargs:\\n- available_functions: \\n- goal: \\n\\n\\n\\n[GOAL]\\n\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n\\n\\n[OUTPUT]\\n'}], extra_body=None), chat_prompt_template=None)}), 'SequentialPlanner_Excluded': KernelPlugin(name='SequentialPlanner_Excluded', description=None, functions={'SequentialPlanner_Excluded': KernelFunction(plugin_name='SequentialPlanner_Excluded', description='Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow', name='SequentialPlanner_Excluded', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969B4C0>, parameters=[ParameterView(name='input', description='The question to answer', default_value='', type_=None, required=False), ParameterView(name='available_functions', description=\"The list of the agent's available_functions\", default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Create an XML plan step by step, to satisfy the goal given, with the available functions.\\n\\n[AVAILABLE FUNCTIONS]\\n\\nSummarizePlugin.MakeAbstractReadable:\\n  description: Given a scientific white paper abstract, rewrite it to make it more readable\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Notegen:\\n  description: Automatically generate compact notes for any text or text document.\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Summarize:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - input: Text to summarize\\n\\nSummarizePlugin.Topics:\\n  description: Analyze given text or document and extract key topics worth remembering\\n  inputs:\\n    - input: \\n\\nWriterPlugin.Acronym:\\n  description: Generate an acronym for the given concept or phrase\\n  inputs:\\n    - input: \\n\\nWriterPlugin.AcronymGenerator:\\n  description: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.AcronymReverse:\\n  description: Given a single word or acronym, generate the expanded form matching the acronym letters.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.Brainstorm:\\n  description: Given a goal or topic description generate a list of ideas\\n  inputs:\\n    - input: A topic description or goal.\\n  - INPUT: \\n\\nWriterPlugin.EmailGen:\\n  description: Write an email from the given bullet points\\n  inputs:\\n    - input: \\n\\nWriterPlugin.EmailTo:\\n  description: Turn bullet points into an email to someone, using a polite tone\\n  inputs:\\n    - to: \\n  - input: \\n  - sender: \\n\\nWriterPlugin.EnglishImprover:\\n  description: Translate text to English and improve it\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.NovelChapter:\\n  description: Write a chapter of a novel.\\n  inputs:\\n    - input: A synopsis of what the chapter should be about.\\n  - theme: The theme or topic of this novel.\\n  - previousChapter: The synopsis of the previous chapter.\\n  - chapterIndex: The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.NovelChapterWithNotes:\\n  description: Write a chapter of a novel using notes about the chapter to write.\\n  inputs:\\n    - input: What the novel should be about.\\n  - theme: The theme of this novel.\\n  - notes: Notes useful to write this chapter.\\n  - previousChapter: The previous chapter synopsis.\\n  - chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline:\\n  description: Generate a list of chapter synopsis for a novel or novella\\n  inputs:\\n    - input: What the novel should be about.\\n  - chapterCount: The number of chapters to generate.\\n  - endMarker: The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.Rewrite:\\n  description: Automatically generate compact notes for any text or text document\\n  inputs:\\n    - style: \\n  - input: \\n\\nWriterPlugin.ShortPoem:\\n  description: Turn a scenario into a short and entertaining poem.\\n  inputs:\\n    - input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen:\\n  description: Generate a list of synopsis for a novel or novella with sub-chapters\\n  inputs:\\n    - input: \\n\\nWriterPlugin.TellMeMore:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - conversationtype: \\n  - input: \\n  - focusarea: \\n  - previousresults: \\n\\nWriterPlugin.Translate:\\n  description: Translate the input into a language of your choice\\n  inputs:\\n    - input: Text to translate\\n  - language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary:\\n  description: Summarize given text in two sentences or less\\n  inputs:\\n    - input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg:\\n  description: Generic function, unknown purpose\\n  inputs:\\n    - available_functions: \\n  - goal: \\n\\nTextPlugin.lowercase:\\n  description: Convert a string to lowercase.\\n  inputs:\\n  \\n\\nTextPlugin.trim:\\n  description: Trim whitespace from the start and end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_end:\\n  description: Trim whitespace from the end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_start:\\n  description: Trim whitespace from the start of a string.\\n  inputs:\\n  \\n\\nTextPlugin.uppercase:\\n  description: Convert a string to uppercase.\\n  inputs:\\n  \\n\\n[END AVAILABLE FUNCTIONS]\\n\\nTo create a plan, follow these steps:\\n0. The plan should be as short as possible.\\n1. From a <goal> create a <plan> as a series of <functions>.\\n2. A plan has \\'INPUT\\' available in context variables by default.\\n3. Before using any function in a plan, check that it is present in the [AVAILABLE FUNCTIONS] list. If it is not, do not use it.\\n4. Only use functions that are required for the given goal.\\n5. Append an \"END\" XML comment at the end of the plan after the final closing </plan> tag.\\n6. Always output valid XML that can be parsed by an XML parser.\\n7. If a plan cannot be created with the [AVAILABLE FUNCTIONS], return <plan />.\\n\\nAll plans take the form of:\\n<plan>\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    (... etc ...)\\n</plan>\\n<!-- END -->\\n\\nTo call a function, follow these steps:\\n1. A function has one or more named parameters and a single \\'output\\' which are all strings. Parameter values should be xml escaped.\\n2. To save an \\'output\\' from a <function>, to pass into a future <function>, use <function.{FullyQualifiedFunctionName} ... setContextVariable=\"<UNIQUE_VARIABLE_KEY>\"/>\\n3. To save an \\'output\\' from a <function>, to return as part of a plan result, use <function.{FullyQualifiedFunctionName} ... appendToResult=\"RESULT__<UNIQUE_RESULT_KEY>\"/>\\n4. Use a \\'$\\' to reference a context variable in a parameter, e.g. when `INPUT=\\'world\\'` the parameter \\'Hello $INPUT\\' will evaluate to `Hello world`.\\n5. Functions do not have access to the context variables of other functions. Do not attempt to use context variables as arrays or objects. Instead, use available functions to extract specific elements or properties from context variables.\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS NOT XML ESCAPED:\\n<function.Name4 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a <!-- \\'comment\\' in it-->\"/>\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS ATTEMPTING TO USE A CONTEXT VARIABLE AS AN ARRAY/OBJECT:\\n<function.CallFunction input=\"$OTHER_OUTPUT[1]\"/>\\n\\nHere is a valid example of how to call a function \"_Function_.Name\" with a single input and save its output:\\n<function._Function_.Name input=\"this is my input\" setContextVariable=\"SOME_KEY\"/>\\n\\nHere is a valid example of how to call a function \"FunctionName2\" with a single input and return its output as part of the plan result:\\n<function.FunctionName2 input=\"Hello $INPUT\" appendToResult=\"RESULT__FINAL_ANSWER\"/>\\n\\nHere is a valid example of how to call a function \"Name3\" with multiple inputs:\\n<function.Name3 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a &lt;!-- &apos;comment&apos; in it--&gt;\"/>\\n\\nBegin!\\n\\n<goal>\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n</goal>\\n'}], extra_body=None), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_vvSKvltRilNtxOYw': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_vvSKvltRilNtxOYw', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969BF70>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'fileIO': KernelPlugin(name='fileIO', description=None, functions={'readAsync': KernelFunction(plugin_name='fileIO', description='Read a file', name='readAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.read of FileIOPlugin()>, parameters=[ParameterView(name='input', description='Path of the source file', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringOutTaskString: 10>, function=<bound method FileIOPlugin.read of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'writeAsync': KernelFunction(plugin_name='fileIO', description='Write a file', name='writeAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.write of FileIOPlugin()>, parameters=[ParameterView(name='content', description='File content', default_value='', type_='string', required=False), ParameterView(name='path', description='Destination path', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InContextOutTask: 16>, function=<bound method FileIOPlugin.write of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'time': KernelPlugin(name='time', description=None, functions={'date': KernelFunction(plugin_name='time', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='time', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='time', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='time', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='time', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='time', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='time', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='time', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='time', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='time', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='time', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='time', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='time', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='time', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='time', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='time', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='time', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='time', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='time', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'text': KernelPlugin(name='text', description=None, functions={...})}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='text', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={'SummarizePlugin': KernelPlugin(name='SummarizePlugin', description=None, functions={'MakeAbstractReadable': KernelFunction(plugin_name='SummarizePlugin', description='Given a scientific white paper abstract, rewrite it to make it more readable', name='MakeAbstractReadable', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7C8E19DC0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 2.0}, ai_model_id=None, frequency_penalty=2.0, logit_bias={}, max_tokens=4000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Notegen': KernelFunction(plugin_name='SummarizePlugin', description='Automatically generate compact notes for any text or text document.', name='Notegen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B0D0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4AF0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Summarize': KernelFunction(plugin_name='SummarizePlugin', description='Summarize given text or any text document', name='Summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E8A4FEE0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E8A7BDC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Topics': KernelFunction(plugin_name='SummarizePlugin', description='Analyze given text or document and extract key topics worth remembering', name='Topics', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B280>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B1F0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 128, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=128, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'Acronym': KernelFunction(plugin_name='WriterPlugin', description='Generate an acronym for the given concept or phrase', name='Acronym', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B940>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9445A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymGenerator': KernelFunction(plugin_name='WriterPlugin', description='Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.', name='AcronymGenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B700>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.7, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymReverse': KernelFunction(plugin_name='WriterPlugin', description='Given a single word or acronym, generate the expanded form matching the acronym letters.', name='AcronymReverse', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BA60>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B9D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.8, 'frequency_penalty': 0.0, 'stop_sequences': ['#END#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.8, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Brainstorm': KernelFunction(plugin_name='WriterPlugin', description='Given a goal or topic description generate a list of ideas', name='Brainstorm', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BDC0>, parameters=[ParameterView(name='input', description='A topic description or goal.', default_value='', type_=None, required=False), ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['##END##']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Must: brainstorm ideas and create a list.\\nMust: use a numbered list.\\nMust: only one list.\\nMust: end list with ##END##\\nShould: no more than 10 items.\\nShould: at least 3 items.\\nTopic: Valentine's Day Date Ideas\\nStart.\\n\"}], extra_body=None), chat_prompt_template=None), 'EmailGen': KernelFunction(plugin_name='WriterPlugin', description='Write an email from the given bullet points', name='EmailGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BD30>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BC10>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EmailTo': KernelFunction(plugin_name='WriterPlugin', description='Turn bullet points into an email to someone, using a polite tone', name='EmailTo', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BEE0>, parameters=[ParameterView(name='to', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='sender', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BE50>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Rewrite my bullet points into an email featuring complete sentences. Use a polite and inclusive tone.  \\n\\n[Input]\\nToby,\\n\\n- Macbeth, King Scotland\\n- Married, Wife Lady Macbeth, No Kids\\n- Dog Toby McDuff. Hunter, dead. \\n- Shakespeare play\\n\\nThanks,\\nDexter\\n\\n+++++\\nHi Toby,\\n\\nThe story of Macbeth\\nMy name is Macbeth. I used to be King of Scotland, but I died. My wife's name is Lady Macbeth and we were married for 15 years. We had no children. Our beloved dog Toby McDuff was a famous hunter of rats in the forest.\\nMy story was immortalized by Shakespeare in a play.\\n\\nThanks,\\nDexter\\n\\n+++++\\n[Input]\\n\\n1. Picnic in the park\\n2. Cooking a romantic dinner together\\n3. Going to a wine tasting\\n4. Ice skating\\n5. Taking a hot air balloon ride\\n6. Watching a romantic movie at home\\n7. Going on a hike and having a picnic at the top\\n8. Taking a dance class together\\n9. Visiting a local museum or art gallery\\n10. Having a spa day together\\n##END##\\n\\nThanks,\\n\\n+++++\\n\"}], extra_body=None), chat_prompt_template=None), 'EnglishImprover': KernelFunction(plugin_name='WriterPlugin', description='Translate text to English and improve it', name='EnglishImprover', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BCA0>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BF70>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 3000, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=3000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapter': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel.', name='NovelChapter', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556040>, parameters=[ParameterView(name='input', description='A synopsis of what the chapter should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme or topic of this novel.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The synopsis of the previous chapter.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556160>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.3, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.3, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapterWithNotes': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel using notes about the chapter to write.', name='NovelChapterWithNotes', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95561F0>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme of this novel.', default_value='', type_=None, required=False), ParameterView(name='notes', description='Notes useful to write this chapter.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The previous chapter synopsis.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95560D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 1024, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelOutline': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of chapter synopsis for a novel or novella', name='NovelOutline', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512A60>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='chapterCount', description='The number of chapters to generate.', default_value='', type_=None, required=False), ParameterView(name='endMarker', description='The marker to use to end each chapter.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512EE0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.1, 'top_p': 0.5, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.1, top_p=0.5, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Rewrite': KernelFunction(plugin_name='WriterPlugin', description='Automatically generate compact notes for any text or text document', name='Rewrite', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512B80>, parameters=[ParameterView(name='style', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512DC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'ShortPoem': KernelFunction(plugin_name='WriterPlugin', description='Turn a scenario into a short and entertaining poem.', name='ShortPoem', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512AF0>, parameters=[ParameterView(name='input', description='The scenario to turn into a poem.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95129D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 60, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=60, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'StoryGen': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of synopsis for a novel or novella with sub-chapters', name='StoryGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556430>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556280>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 250, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=250, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'TellMeMore': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='TellMeMore', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95564C0>, parameters=[ParameterView(name='conversationtype', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='focusarea', description='', default_value='', type_='string', required=False), ParameterView(name='previousresults', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556310>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 500, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=500, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Translate': KernelFunction(plugin_name='WriterPlugin', description='Translate the input into a language of your choice', name='Translate', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556670>, parameters=[ParameterView(name='input', description='Text to translate', default_value='', type_=None, required=False), ParameterView(name='language', description='Language to translate to', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95565E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.7, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['[done]']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Translate the input below into English\\n\\nMAKE SURE YOU ONLY USE English.\\n\\nHi there,\\n\\nI hope this email finds you well. I wanted to share some ideas for fun and romantic activities that we could do together:\\n\\n1. We could have a lovely picnic in the park, surrounded by nature and fresh air.\\n2. Cooking a romantic dinner together could be a fun and intimate way to spend an evening.\\n3. Going to a wine tasting would be a great opportunity to try new wines and learn more about them.\\n4. Ice skating is always a fun and playful activity that we could enjoy together.\\n5. Taking a hot air balloon ride would be a unique and exciting experience that we could share.\\n6. We could also have a cozy night in and watch a romantic movie together.\\n7. Going on a hike and having a picnic at the top would be a great way to enjoy the outdoors and each other's company.\\n8. Taking a dance class together could be a fun and romantic way to learn something new.\\n9. Visiting a local museum or art gallery would be a great way to appreciate art and culture together.\\n10. Finally, we could have a relaxing spa day together and enjoy some pampering and relaxation.\\n\\nLet me know what you think and if you have any other ideas!\\n\\nThanks,\\n[Your Name]\\n\\nTranslation:\\n\"}], extra_body=None), chat_prompt_template=None), 'TwoSentenceSummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in two sentences or less', name='TwoSentenceSummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95563A0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556790>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'TextPlugin': KernelPlugin(name='TextPlugin', description=None, functions={'lowercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_tSNsOTIMHWAhndYw': KernelPlugin(name='p_tSNsOTIMHWAhndYw', description=None, functions={'f_QOkQhOcvcIJkhbYg': KernelFunction(plugin_name='p_tSNsOTIMHWAhndYw', description='Generic function, unknown purpose', name='f_QOkQhOcvcIJkhbYg', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E964DF70>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B550>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\\nYou are a planner for the Semantic Kernel.\\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\\nCreate a list of subtasks based off the [GOAL] provided.\\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\\nBase your decisions on which functions to use from the description and the name of the function.\\nSometimes, a function may take arguments. Provide them if necessary.\\nThe plan should be as short as possible.\\nFor example:\\n\\n[AVAILABLE FUNCTIONS]\\nEmailConnector.LookupContactEmail\\ndescription: looks up the a contact and retrieves their email address\\nargs:\\n- name: the name to look up\\n\\nWriterPlugin.EmailTo\\ndescription: email the input text to a recipient\\nargs:\\n- input: the text to email\\n- recipient: the recipient\\'s email address. Multiple addresses may be included if separated by \\';\\'.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\nWriterPlugin.Summarize\\ndescription: summarize input text\\nargs:\\n- input: the text to summarize\\n\\nFunPlugin.Joke\\ndescription: Generate a funny joke\\nargs:\\n- input: the input to generate a joke about\\n\\n[GOAL]\\n\"Tell a joke about cars. Translate it to Spanish\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"cars\",\\n        \"subtasks\": [\\n            {\"function\": \"FunPlugin.Joke\"},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"Spanish\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nWriterPlugin.Brainstorm\\ndescription: Brainstorm ideas\\nargs:\\n- input: the input to brainstorm about\\n\\nEdgarAllenPoePlugin.Poe\\ndescription: Write in the style of author Edgar Allen Poe\\nargs:\\n- input: the input to write about\\n\\nWriterPlugin.EmailTo\\ndescription: Write an email to a recipient\\nargs:\\n- input: the input to write about\\n- recipient: the recipient\\'s email address.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\n[GOAL]\\n\"Tomorrow is Valentine\\'s day. I need to come up with a few date ideas.\\nShe likes Edgar Allen Poe so write using his style.\\nE-mail these ideas to my significant other. Translate it to French.\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"Valentine\\'s Day Date Ideas\",\\n        \"subtasks\": [\\n            {\"function\": \"WriterPlugin.Brainstorm\"},\\n            {\"function\": \"EdgarAllenPoePlugin.Poe\"},\\n            {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"French\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nTextPlugin.lowercase\\ndescription: Convert a string to lowercase.\\nargs:\\n\\nTextPlugin.trim\\ndescription: Trim whitespace from the start and end of a string.\\nargs:\\n\\nTextPlugin.trim_end\\ndescription: Trim whitespace from the end of a string.\\nargs:\\n\\nTextPlugin.trim_start\\ndescription: Trim whitespace from the start of a string.\\nargs:\\n\\nTextPlugin.uppercase\\ndescription: Convert a string to uppercase.\\nargs:\\n\\nSummarizePlugin.MakeAbstractReadable\\ndescription: Given a scientific white paper abstract, rewrite it to make it more readable\\nargs:\\n- input: \\n\\nSummarizePlugin.Notegen\\ndescription: Automatically generate compact notes for any text or text document.\\nargs:\\n- input: \\n\\nSummarizePlugin.Summarize\\ndescription: Summarize given text or any text document\\nargs:\\n- input: Text to summarize\\n\\nSummarizePlugin.Topics\\ndescription: Analyze given text or document and extract key topics worth remembering\\nargs:\\n- input: \\n\\nWriterPlugin.Acronym\\ndescription: Generate an acronym for the given concept or phrase\\nargs:\\n- input: \\n\\nWriterPlugin.AcronymGenerator\\ndescription: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.AcronymReverse\\ndescription: Given a single word or acronym, generate the expanded form matching the acronym letters.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.Brainstorm\\ndescription: Given a goal or topic description generate a list of ideas\\nargs:\\n- input: A topic description or goal.\\n- INPUT: \\n\\nWriterPlugin.EmailGen\\ndescription: Write an email from the given bullet points\\nargs:\\n- input: \\n\\nWriterPlugin.EmailTo\\ndescription: Turn bullet points into an email to someone, using a polite tone\\nargs:\\n- to: \\n- input: \\n- sender: \\n\\nWriterPlugin.EnglishImprover\\ndescription: Translate text to English and improve it\\nargs:\\n- INPUT: \\n\\nWriterPlugin.NovelChapter\\ndescription: Write a chapter of a novel.\\nargs:\\n- input: A synopsis of what the chapter should be about.\\n- theme: The theme or topic of this novel.\\n- previousChapter: The synopsis of the previous chapter.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelChapterWithNotes\\ndescription: Write a chapter of a novel using notes about the chapter to write.\\nargs:\\n- input: What the novel should be about.\\n- theme: The theme of this novel.\\n- notes: Notes useful to write this chapter.\\n- previousChapter: The previous chapter synopsis.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline\\ndescription: Generate a list of chapter synopsis for a novel or novella\\nargs:\\n- input: What the novel should be about.\\n- chapterCount: The number of chapters to generate.\\n- endMarker: The marker to use to end each chapter.\\n\\nWriterPlugin.Rewrite\\ndescription: Automatically generate compact notes for any text or text document\\nargs:\\n- style: \\n- input: \\n\\nWriterPlugin.ShortPoem\\ndescription: Turn a scenario into a short and entertaining poem.\\nargs:\\n- input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen\\ndescription: Generate a list of synopsis for a novel or novella with sub-chapters\\nargs:\\n- input: \\n\\nWriterPlugin.TellMeMore\\ndescription: Summarize given text or any text document\\nargs:\\n- conversationtype: \\n- input: \\n- focusarea: \\n- previousresults: \\n\\nWriterPlugin.Translate\\ndescription: Translate the input into a language of your choice\\nargs:\\n- input: Text to translate\\n- language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary\\ndescription: Summarize given text in two sentences or less\\nargs:\\n- input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\ndescription: Generic function, unknown purpose\\nargs:\\n- available_functions: \\n- goal: \\n\\n\\n\\n[GOAL]\\n\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n\\n\\n[OUTPUT]\\n'}], extra_body=None), chat_prompt_template=None)}), 'SequentialPlanner_Excluded': KernelPlugin(name='SequentialPlanner_Excluded', description=None, functions={'SequentialPlanner_Excluded': KernelFunction(plugin_name='SequentialPlanner_Excluded', description='Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow', name='SequentialPlanner_Excluded', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969B4C0>, parameters=[ParameterView(name='input', description='The question to answer', default_value='', type_=None, required=False), ParameterView(name='available_functions', description=\"The list of the agent's available_functions\", default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Create an XML plan step by step, to satisfy the goal given, with the available functions.\\n\\n[AVAILABLE FUNCTIONS]\\n\\nSummarizePlugin.MakeAbstractReadable:\\n  description: Given a scientific white paper abstract, rewrite it to make it more readable\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Notegen:\\n  description: Automatically generate compact notes for any text or text document.\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Summarize:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - input: Text to summarize\\n\\nSummarizePlugin.Topics:\\n  description: Analyze given text or document and extract key topics worth remembering\\n  inputs:\\n    - input: \\n\\nWriterPlugin.Acronym:\\n  description: Generate an acronym for the given concept or phrase\\n  inputs:\\n    - input: \\n\\nWriterPlugin.AcronymGenerator:\\n  description: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.AcronymReverse:\\n  description: Given a single word or acronym, generate the expanded form matching the acronym letters.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.Brainstorm:\\n  description: Given a goal or topic description generate a list of ideas\\n  inputs:\\n    - input: A topic description or goal.\\n  - INPUT: \\n\\nWriterPlugin.EmailGen:\\n  description: Write an email from the given bullet points\\n  inputs:\\n    - input: \\n\\nWriterPlugin.EmailTo:\\n  description: Turn bullet points into an email to someone, using a polite tone\\n  inputs:\\n    - to: \\n  - input: \\n  - sender: \\n\\nWriterPlugin.EnglishImprover:\\n  description: Translate text to English and improve it\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.NovelChapter:\\n  description: Write a chapter of a novel.\\n  inputs:\\n    - input: A synopsis of what the chapter should be about.\\n  - theme: The theme or topic of this novel.\\n  - previousChapter: The synopsis of the previous chapter.\\n  - chapterIndex: The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.NovelChapterWithNotes:\\n  description: Write a chapter of a novel using notes about the chapter to write.\\n  inputs:\\n    - input: What the novel should be about.\\n  - theme: The theme of this novel.\\n  - notes: Notes useful to write this chapter.\\n  - previousChapter: The previous chapter synopsis.\\n  - chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline:\\n  description: Generate a list of chapter synopsis for a novel or novella\\n  inputs:\\n    - input: What the novel should be about.\\n  - chapterCount: The number of chapters to generate.\\n  - endMarker: The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.Rewrite:\\n  description: Automatically generate compact notes for any text or text document\\n  inputs:\\n    - style: \\n  - input: \\n\\nWriterPlugin.ShortPoem:\\n  description: Turn a scenario into a short and entertaining poem.\\n  inputs:\\n    - input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen:\\n  description: Generate a list of synopsis for a novel or novella with sub-chapters\\n  inputs:\\n    - input: \\n\\nWriterPlugin.TellMeMore:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - conversationtype: \\n  - input: \\n  - focusarea: \\n  - previousresults: \\n\\nWriterPlugin.Translate:\\n  description: Translate the input into a language of your choice\\n  inputs:\\n    - input: Text to translate\\n  - language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary:\\n  description: Summarize given text in two sentences or less\\n  inputs:\\n    - input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg:\\n  description: Generic function, unknown purpose\\n  inputs:\\n    - available_functions: \\n  - goal: \\n\\nTextPlugin.lowercase:\\n  description: Convert a string to lowercase.\\n  inputs:\\n  \\n\\nTextPlugin.trim:\\n  description: Trim whitespace from the start and end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_end:\\n  description: Trim whitespace from the end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_start:\\n  description: Trim whitespace from the start of a string.\\n  inputs:\\n  \\n\\nTextPlugin.uppercase:\\n  description: Convert a string to uppercase.\\n  inputs:\\n  \\n\\n[END AVAILABLE FUNCTIONS]\\n\\nTo create a plan, follow these steps:\\n0. The plan should be as short as possible.\\n1. From a <goal> create a <plan> as a series of <functions>.\\n2. A plan has \\'INPUT\\' available in context variables by default.\\n3. Before using any function in a plan, check that it is present in the [AVAILABLE FUNCTIONS] list. If it is not, do not use it.\\n4. Only use functions that are required for the given goal.\\n5. Append an \"END\" XML comment at the end of the plan after the final closing </plan> tag.\\n6. Always output valid XML that can be parsed by an XML parser.\\n7. If a plan cannot be created with the [AVAILABLE FUNCTIONS], return <plan />.\\n\\nAll plans take the form of:\\n<plan>\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    (... etc ...)\\n</plan>\\n<!-- END -->\\n\\nTo call a function, follow these steps:\\n1. A function has one or more named parameters and a single \\'output\\' which are all strings. Parameter values should be xml escaped.\\n2. To save an \\'output\\' from a <function>, to pass into a future <function>, use <function.{FullyQualifiedFunctionName} ... setContextVariable=\"<UNIQUE_VARIABLE_KEY>\"/>\\n3. To save an \\'output\\' from a <function>, to return as part of a plan result, use <function.{FullyQualifiedFunctionName} ... appendToResult=\"RESULT__<UNIQUE_RESULT_KEY>\"/>\\n4. Use a \\'$\\' to reference a context variable in a parameter, e.g. when `INPUT=\\'world\\'` the parameter \\'Hello $INPUT\\' will evaluate to `Hello world`.\\n5. Functions do not have access to the context variables of other functions. Do not attempt to use context variables as arrays or objects. Instead, use available functions to extract specific elements or properties from context variables.\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS NOT XML ESCAPED:\\n<function.Name4 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a <!-- \\'comment\\' in it-->\"/>\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS ATTEMPTING TO USE A CONTEXT VARIABLE AS AN ARRAY/OBJECT:\\n<function.CallFunction input=\"$OTHER_OUTPUT[1]\"/>\\n\\nHere is a valid example of how to call a function \"_Function_.Name\" with a single input and save its output:\\n<function._Function_.Name input=\"this is my input\" setContextVariable=\"SOME_KEY\"/>\\n\\nHere is a valid example of how to call a function \"FunctionName2\" with a single input and return its output as part of the plan result:\\n<function.FunctionName2 input=\"Hello $INPUT\" appendToResult=\"RESULT__FINAL_ANSWER\"/>\\n\\nHere is a valid example of how to call a function \"Name3\" with multiple inputs:\\n<function.Name3 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a &lt;!-- &apos;comment&apos; in it--&gt;\"/>\\n\\nBegin!\\n\\n<goal>\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n</goal>\\n'}], extra_body=None), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_vvSKvltRilNtxOYw': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_vvSKvltRilNtxOYw', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969BF70>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'fileIO': KernelPlugin(name='fileIO', description=None, functions={'readAsync': KernelFunction(plugin_name='fileIO', description='Read a file', name='readAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.read of FileIOPlugin()>, parameters=[ParameterView(name='input', description='Path of the source file', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringOutTaskString: 10>, function=<bound method FileIOPlugin.read of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'writeAsync': KernelFunction(plugin_name='fileIO', description='Write a file', name='writeAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.write of FileIOPlugin()>, parameters=[ParameterView(name='content', description='File content', default_value='', type_='string', required=False), ParameterView(name='path', description='Destination path', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InContextOutTask: 16>, function=<bound method FileIOPlugin.write of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'time': KernelPlugin(name='time', description=None, functions={'date': KernelFunction(plugin_name='time', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='time', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='time', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='time', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='time', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='time', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='time', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='time', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='time', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='time', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='time', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='time', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='time', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='time', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='time', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='time', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='time', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='time', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='time', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'text': KernelPlugin(name='text', description=None, functions={...})}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='text', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={'SummarizePlugin': KernelPlugin(name='SummarizePlugin', description=None, functions={'MakeAbstractReadable': KernelFunction(plugin_name='SummarizePlugin', description='Given a scientific white paper abstract, rewrite it to make it more readable', name='MakeAbstractReadable', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7C8E19DC0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 2.0}, ai_model_id=None, frequency_penalty=2.0, logit_bias={}, max_tokens=4000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Notegen': KernelFunction(plugin_name='SummarizePlugin', description='Automatically generate compact notes for any text or text document.', name='Notegen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B0D0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4AF0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Summarize': KernelFunction(plugin_name='SummarizePlugin', description='Summarize given text or any text document', name='Summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E8A4FEE0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E8A7BDC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Topics': KernelFunction(plugin_name='SummarizePlugin', description='Analyze given text or document and extract key topics worth remembering', name='Topics', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B280>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B1F0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 128, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=128, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'Acronym': KernelFunction(plugin_name='WriterPlugin', description='Generate an acronym for the given concept or phrase', name='Acronym', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B940>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9445A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymGenerator': KernelFunction(plugin_name='WriterPlugin', description='Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.', name='AcronymGenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B700>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.7, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymReverse': KernelFunction(plugin_name='WriterPlugin', description='Given a single word or acronym, generate the expanded form matching the acronym letters.', name='AcronymReverse', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BA60>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B9D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.8, 'frequency_penalty': 0.0, 'stop_sequences': ['#END#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.8, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Brainstorm': KernelFunction(plugin_name='WriterPlugin', description='Given a goal or topic description generate a list of ideas', name='Brainstorm', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BDC0>, parameters=[ParameterView(name='input', description='A topic description or goal.', default_value='', type_=None, required=False), ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['##END##']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Must: brainstorm ideas and create a list.\\nMust: use a numbered list.\\nMust: only one list.\\nMust: end list with ##END##\\nShould: no more than 10 items.\\nShould: at least 3 items.\\nTopic: Valentine's Day Date Ideas\\nStart.\\n\"}], extra_body=None), chat_prompt_template=None), 'EmailGen': KernelFunction(plugin_name='WriterPlugin', description='Write an email from the given bullet points', name='EmailGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BD30>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BC10>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EmailTo': KernelFunction(plugin_name='WriterPlugin', description='Turn bullet points into an email to someone, using a polite tone', name='EmailTo', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BEE0>, parameters=[ParameterView(name='to', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='sender', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BE50>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Rewrite my bullet points into an email featuring complete sentences. Use a polite and inclusive tone.  \\n\\n[Input]\\nToby,\\n\\n- Macbeth, King Scotland\\n- Married, Wife Lady Macbeth, No Kids\\n- Dog Toby McDuff. Hunter, dead. \\n- Shakespeare play\\n\\nThanks,\\nDexter\\n\\n+++++\\nHi Toby,\\n\\nThe story of Macbeth\\nMy name is Macbeth. I used to be King of Scotland, but I died. My wife's name is Lady Macbeth and we were married for 15 years. We had no children. Our beloved dog Toby McDuff was a famous hunter of rats in the forest.\\nMy story was immortalized by Shakespeare in a play.\\n\\nThanks,\\nDexter\\n\\n+++++\\n[Input]\\n\\n1. Picnic in the park\\n2. Cooking a romantic dinner together\\n3. Going to a wine tasting\\n4. Ice skating\\n5. Taking a hot air balloon ride\\n6. Watching a romantic movie at home\\n7. Going on a hike and having a picnic at the top\\n8. Taking a dance class together\\n9. Visiting a local museum or art gallery\\n10. Having a spa day together\\n##END##\\n\\nThanks,\\n\\n+++++\\n\"}], extra_body=None), chat_prompt_template=None), 'EnglishImprover': KernelFunction(plugin_name='WriterPlugin', description='Translate text to English and improve it', name='EnglishImprover', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BCA0>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BF70>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 3000, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=3000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapter': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel.', name='NovelChapter', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556040>, parameters=[ParameterView(name='input', description='A synopsis of what the chapter should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme or topic of this novel.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The synopsis of the previous chapter.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556160>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.3, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.3, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapterWithNotes': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel using notes about the chapter to write.', name='NovelChapterWithNotes', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95561F0>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme of this novel.', default_value='', type_=None, required=False), ParameterView(name='notes', description='Notes useful to write this chapter.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The previous chapter synopsis.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95560D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 1024, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelOutline': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of chapter synopsis for a novel or novella', name='NovelOutline', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512A60>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='chapterCount', description='The number of chapters to generate.', default_value='', type_=None, required=False), ParameterView(name='endMarker', description='The marker to use to end each chapter.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512EE0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.1, 'top_p': 0.5, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.1, top_p=0.5, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Rewrite': KernelFunction(plugin_name='WriterPlugin', description='Automatically generate compact notes for any text or text document', name='Rewrite', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512B80>, parameters=[ParameterView(name='style', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512DC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'ShortPoem': KernelFunction(plugin_name='WriterPlugin', description='Turn a scenario into a short and entertaining poem.', name='ShortPoem', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512AF0>, parameters=[ParameterView(name='input', description='The scenario to turn into a poem.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95129D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 60, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=60, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'StoryGen': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of synopsis for a novel or novella with sub-chapters', name='StoryGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556430>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556280>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 250, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=250, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'TellMeMore': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='TellMeMore', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95564C0>, parameters=[ParameterView(name='conversationtype', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='focusarea', description='', default_value='', type_='string', required=False), ParameterView(name='previousresults', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556310>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 500, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=500, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Translate': KernelFunction(plugin_name='WriterPlugin', description='Translate the input into a language of your choice', name='Translate', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556670>, parameters=[ParameterView(name='input', description='Text to translate', default_value='', type_=None, required=False), ParameterView(name='language', description='Language to translate to', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95565E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.7, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['[done]']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Translate the input below into English\\n\\nMAKE SURE YOU ONLY USE English.\\n\\nHi there,\\n\\nI hope this email finds you well. I wanted to share some ideas for fun and romantic activities that we could do together:\\n\\n1. We could have a lovely picnic in the park, surrounded by nature and fresh air.\\n2. Cooking a romantic dinner together could be a fun and intimate way to spend an evening.\\n3. Going to a wine tasting would be a great opportunity to try new wines and learn more about them.\\n4. Ice skating is always a fun and playful activity that we could enjoy together.\\n5. Taking a hot air balloon ride would be a unique and exciting experience that we could share.\\n6. We could also have a cozy night in and watch a romantic movie together.\\n7. Going on a hike and having a picnic at the top would be a great way to enjoy the outdoors and each other's company.\\n8. Taking a dance class together could be a fun and romantic way to learn something new.\\n9. Visiting a local museum or art gallery would be a great way to appreciate art and culture together.\\n10. Finally, we could have a relaxing spa day together and enjoy some pampering and relaxation.\\n\\nLet me know what you think and if you have any other ideas!\\n\\nThanks,\\n[Your Name]\\n\\nTranslation:\\n\"}], extra_body=None), chat_prompt_template=None), 'TwoSentenceSummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in two sentences or less', name='TwoSentenceSummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95563A0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556790>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'TextPlugin': KernelPlugin(name='TextPlugin', description=None, functions={'lowercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_tSNsOTIMHWAhndYw': KernelPlugin(name='p_tSNsOTIMHWAhndYw', description=None, functions={'f_QOkQhOcvcIJkhbYg': KernelFunction(plugin_name='p_tSNsOTIMHWAhndYw', description='Generic function, unknown purpose', name='f_QOkQhOcvcIJkhbYg', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E964DF70>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B550>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\\nYou are a planner for the Semantic Kernel.\\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\\nCreate a list of subtasks based off the [GOAL] provided.\\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\\nBase your decisions on which functions to use from the description and the name of the function.\\nSometimes, a function may take arguments. Provide them if necessary.\\nThe plan should be as short as possible.\\nFor example:\\n\\n[AVAILABLE FUNCTIONS]\\nEmailConnector.LookupContactEmail\\ndescription: looks up the a contact and retrieves their email address\\nargs:\\n- name: the name to look up\\n\\nWriterPlugin.EmailTo\\ndescription: email the input text to a recipient\\nargs:\\n- input: the text to email\\n- recipient: the recipient\\'s email address. Multiple addresses may be included if separated by \\';\\'.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\nWriterPlugin.Summarize\\ndescription: summarize input text\\nargs:\\n- input: the text to summarize\\n\\nFunPlugin.Joke\\ndescription: Generate a funny joke\\nargs:\\n- input: the input to generate a joke about\\n\\n[GOAL]\\n\"Tell a joke about cars. Translate it to Spanish\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"cars\",\\n        \"subtasks\": [\\n            {\"function\": \"FunPlugin.Joke\"},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"Spanish\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nWriterPlugin.Brainstorm\\ndescription: Brainstorm ideas\\nargs:\\n- input: the input to brainstorm about\\n\\nEdgarAllenPoePlugin.Poe\\ndescription: Write in the style of author Edgar Allen Poe\\nargs:\\n- input: the input to write about\\n\\nWriterPlugin.EmailTo\\ndescription: Write an email to a recipient\\nargs:\\n- input: the input to write about\\n- recipient: the recipient\\'s email address.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\n[GOAL]\\n\"Tomorrow is Valentine\\'s day. I need to come up with a few date ideas.\\nShe likes Edgar Allen Poe so write using his style.\\nE-mail these ideas to my significant other. Translate it to French.\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"Valentine\\'s Day Date Ideas\",\\n        \"subtasks\": [\\n            {\"function\": \"WriterPlugin.Brainstorm\"},\\n            {\"function\": \"EdgarAllenPoePlugin.Poe\"},\\n            {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"French\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nTextPlugin.lowercase\\ndescription: Convert a string to lowercase.\\nargs:\\n\\nTextPlugin.trim\\ndescription: Trim whitespace from the start and end of a string.\\nargs:\\n\\nTextPlugin.trim_end\\ndescription: Trim whitespace from the end of a string.\\nargs:\\n\\nTextPlugin.trim_start\\ndescription: Trim whitespace from the start of a string.\\nargs:\\n\\nTextPlugin.uppercase\\ndescription: Convert a string to uppercase.\\nargs:\\n\\nSummarizePlugin.MakeAbstractReadable\\ndescription: Given a scientific white paper abstract, rewrite it to make it more readable\\nargs:\\n- input: \\n\\nSummarizePlugin.Notegen\\ndescription: Automatically generate compact notes for any text or text document.\\nargs:\\n- input: \\n\\nSummarizePlugin.Summarize\\ndescription: Summarize given text or any text document\\nargs:\\n- input: Text to summarize\\n\\nSummarizePlugin.Topics\\ndescription: Analyze given text or document and extract key topics worth remembering\\nargs:\\n- input: \\n\\nWriterPlugin.Acronym\\ndescription: Generate an acronym for the given concept or phrase\\nargs:\\n- input: \\n\\nWriterPlugin.AcronymGenerator\\ndescription: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.AcronymReverse\\ndescription: Given a single word or acronym, generate the expanded form matching the acronym letters.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.Brainstorm\\ndescription: Given a goal or topic description generate a list of ideas\\nargs:\\n- input: A topic description or goal.\\n- INPUT: \\n\\nWriterPlugin.EmailGen\\ndescription: Write an email from the given bullet points\\nargs:\\n- input: \\n\\nWriterPlugin.EmailTo\\ndescription: Turn bullet points into an email to someone, using a polite tone\\nargs:\\n- to: \\n- input: \\n- sender: \\n\\nWriterPlugin.EnglishImprover\\ndescription: Translate text to English and improve it\\nargs:\\n- INPUT: \\n\\nWriterPlugin.NovelChapter\\ndescription: Write a chapter of a novel.\\nargs:\\n- input: A synopsis of what the chapter should be about.\\n- theme: The theme or topic of this novel.\\n- previousChapter: The synopsis of the previous chapter.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelChapterWithNotes\\ndescription: Write a chapter of a novel using notes about the chapter to write.\\nargs:\\n- input: What the novel should be about.\\n- theme: The theme of this novel.\\n- notes: Notes useful to write this chapter.\\n- previousChapter: The previous chapter synopsis.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline\\ndescription: Generate a list of chapter synopsis for a novel or novella\\nargs:\\n- input: What the novel should be about.\\n- chapterCount: The number of chapters to generate.\\n- endMarker: The marker to use to end each chapter.\\n\\nWriterPlugin.Rewrite\\ndescription: Automatically generate compact notes for any text or text document\\nargs:\\n- style: \\n- input: \\n\\nWriterPlugin.ShortPoem\\ndescription: Turn a scenario into a short and entertaining poem.\\nargs:\\n- input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen\\ndescription: Generate a list of synopsis for a novel or novella with sub-chapters\\nargs:\\n- input: \\n\\nWriterPlugin.TellMeMore\\ndescription: Summarize given text or any text document\\nargs:\\n- conversationtype: \\n- input: \\n- focusarea: \\n- previousresults: \\n\\nWriterPlugin.Translate\\ndescription: Translate the input into a language of your choice\\nargs:\\n- input: Text to translate\\n- language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary\\ndescription: Summarize given text in two sentences or less\\nargs:\\n- input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\ndescription: Generic function, unknown purpose\\nargs:\\n- available_functions: \\n- goal: \\n\\n\\n\\n[GOAL]\\n\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n\\n\\n[OUTPUT]\\n'}], extra_body=None), chat_prompt_template=None)}), 'SequentialPlanner_Excluded': KernelPlugin(name='SequentialPlanner_Excluded', description=None, functions={'SequentialPlanner_Excluded': KernelFunction(plugin_name='SequentialPlanner_Excluded', description='Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow', name='SequentialPlanner_Excluded', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969B4C0>, parameters=[ParameterView(name='input', description='The question to answer', default_value='', type_=None, required=False), ParameterView(name='available_functions', description=\"The list of the agent's available_functions\", default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Create an XML plan step by step, to satisfy the goal given, with the available functions.\\n\\n[AVAILABLE FUNCTIONS]\\n\\nSummarizePlugin.MakeAbstractReadable:\\n  description: Given a scientific white paper abstract, rewrite it to make it more readable\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Notegen:\\n  description: Automatically generate compact notes for any text or text document.\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Summarize:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - input: Text to summarize\\n\\nSummarizePlugin.Topics:\\n  description: Analyze given text or document and extract key topics worth remembering\\n  inputs:\\n    - input: \\n\\nWriterPlugin.Acronym:\\n  description: Generate an acronym for the given concept or phrase\\n  inputs:\\n    - input: \\n\\nWriterPlugin.AcronymGenerator:\\n  description: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.AcronymReverse:\\n  description: Given a single word or acronym, generate the expanded form matching the acronym letters.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.Brainstorm:\\n  description: Given a goal or topic description generate a list of ideas\\n  inputs:\\n    - input: A topic description or goal.\\n  - INPUT: \\n\\nWriterPlugin.EmailGen:\\n  description: Write an email from the given bullet points\\n  inputs:\\n    - input: \\n\\nWriterPlugin.EmailTo:\\n  description: Turn bullet points into an email to someone, using a polite tone\\n  inputs:\\n    - to: \\n  - input: \\n  - sender: \\n\\nWriterPlugin.EnglishImprover:\\n  description: Translate text to English and improve it\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.NovelChapter:\\n  description: Write a chapter of a novel.\\n  inputs:\\n    - input: A synopsis of what the chapter should be about.\\n  - theme: The theme or topic of this novel.\\n  - previousChapter: The synopsis of the previous chapter.\\n  - chapterIndex: The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.NovelChapterWithNotes:\\n  description: Write a chapter of a novel using notes about the chapter to write.\\n  inputs:\\n    - input: What the novel should be about.\\n  - theme: The theme of this novel.\\n  - notes: Notes useful to write this chapter.\\n  - previousChapter: The previous chapter synopsis.\\n  - chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline:\\n  description: Generate a list of chapter synopsis for a novel or novella\\n  inputs:\\n    - input: What the novel should be about.\\n  - chapterCount: The number of chapters to generate.\\n  - endMarker: The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.Rewrite:\\n  description: Automatically generate compact notes for any text or text document\\n  inputs:\\n    - style: \\n  - input: \\n\\nWriterPlugin.ShortPoem:\\n  description: Turn a scenario into a short and entertaining poem.\\n  inputs:\\n    - input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen:\\n  description: Generate a list of synopsis for a novel or novella with sub-chapters\\n  inputs:\\n    - input: \\n\\nWriterPlugin.TellMeMore:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - conversationtype: \\n  - input: \\n  - focusarea: \\n  - previousresults: \\n\\nWriterPlugin.Translate:\\n  description: Translate the input into a language of your choice\\n  inputs:\\n    - input: Text to translate\\n  - language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary:\\n  description: Summarize given text in two sentences or less\\n  inputs:\\n    - input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg:\\n  description: Generic function, unknown purpose\\n  inputs:\\n    - available_functions: \\n  - goal: \\n\\nTextPlugin.lowercase:\\n  description: Convert a string to lowercase.\\n  inputs:\\n  \\n\\nTextPlugin.trim:\\n  description: Trim whitespace from the start and end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_end:\\n  description: Trim whitespace from the end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_start:\\n  description: Trim whitespace from the start of a string.\\n  inputs:\\n  \\n\\nTextPlugin.uppercase:\\n  description: Convert a string to uppercase.\\n  inputs:\\n  \\n\\n[END AVAILABLE FUNCTIONS]\\n\\nTo create a plan, follow these steps:\\n0. The plan should be as short as possible.\\n1. From a <goal> create a <plan> as a series of <functions>.\\n2. A plan has \\'INPUT\\' available in context variables by default.\\n3. Before using any function in a plan, check that it is present in the [AVAILABLE FUNCTIONS] list. If it is not, do not use it.\\n4. Only use functions that are required for the given goal.\\n5. Append an \"END\" XML comment at the end of the plan after the final closing </plan> tag.\\n6. Always output valid XML that can be parsed by an XML parser.\\n7. If a plan cannot be created with the [AVAILABLE FUNCTIONS], return <plan />.\\n\\nAll plans take the form of:\\n<plan>\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    (... etc ...)\\n</plan>\\n<!-- END -->\\n\\nTo call a function, follow these steps:\\n1. A function has one or more named parameters and a single \\'output\\' which are all strings. Parameter values should be xml escaped.\\n2. To save an \\'output\\' from a <function>, to pass into a future <function>, use <function.{FullyQualifiedFunctionName} ... setContextVariable=\"<UNIQUE_VARIABLE_KEY>\"/>\\n3. To save an \\'output\\' from a <function>, to return as part of a plan result, use <function.{FullyQualifiedFunctionName} ... appendToResult=\"RESULT__<UNIQUE_RESULT_KEY>\"/>\\n4. Use a \\'$\\' to reference a context variable in a parameter, e.g. when `INPUT=\\'world\\'` the parameter \\'Hello $INPUT\\' will evaluate to `Hello world`.\\n5. Functions do not have access to the context variables of other functions. Do not attempt to use context variables as arrays or objects. Instead, use available functions to extract specific elements or properties from context variables.\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS NOT XML ESCAPED:\\n<function.Name4 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a <!-- \\'comment\\' in it-->\"/>\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS ATTEMPTING TO USE A CONTEXT VARIABLE AS AN ARRAY/OBJECT:\\n<function.CallFunction input=\"$OTHER_OUTPUT[1]\"/>\\n\\nHere is a valid example of how to call a function \"_Function_.Name\" with a single input and save its output:\\n<function._Function_.Name input=\"this is my input\" setContextVariable=\"SOME_KEY\"/>\\n\\nHere is a valid example of how to call a function \"FunctionName2\" with a single input and return its output as part of the plan result:\\n<function.FunctionName2 input=\"Hello $INPUT\" appendToResult=\"RESULT__FINAL_ANSWER\"/>\\n\\nHere is a valid example of how to call a function \"Name3\" with multiple inputs:\\n<function.Name3 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a &lt;!-- &apos;comment&apos; in it--&gt;\"/>\\n\\nBegin!\\n\\n<goal>\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n</goal>\\n'}], extra_body=None), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_vvSKvltRilNtxOYw': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_vvSKvltRilNtxOYw', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969BF70>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'fileIO': KernelPlugin(name='fileIO', description=None, functions={'readAsync': KernelFunction(plugin_name='fileIO', description='Read a file', name='readAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.read of FileIOPlugin()>, parameters=[ParameterView(name='input', description='Path of the source file', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringOutTaskString: 10>, function=<bound method FileIOPlugin.read of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'writeAsync': KernelFunction(plugin_name='fileIO', description='Write a file', name='writeAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.write of FileIOPlugin()>, parameters=[ParameterView(name='content', description='File content', default_value='', type_='string', required=False), ParameterView(name='path', description='Destination path', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InContextOutTask: 16>, function=<bound method FileIOPlugin.write of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'time': KernelPlugin(name='time', description=None, functions={'date': KernelFunction(plugin_name='time', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='time', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='time', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='time', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='time', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='time', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='time', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='time', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='time', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='time', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='time', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='time', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='time', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='time', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='time', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='time', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='time', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='time', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='time', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'text': KernelPlugin(name='text', description=None, functions={...})}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='text', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={'SummarizePlugin': KernelPlugin(name='SummarizePlugin', description=None, functions={'MakeAbstractReadable': KernelFunction(plugin_name='SummarizePlugin', description='Given a scientific white paper abstract, rewrite it to make it more readable', name='MakeAbstractReadable', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7C8E19DC0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 2.0}, ai_model_id=None, frequency_penalty=2.0, logit_bias={}, max_tokens=4000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Notegen': KernelFunction(plugin_name='SummarizePlugin', description='Automatically generate compact notes for any text or text document.', name='Notegen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B0D0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4AF0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Summarize': KernelFunction(plugin_name='SummarizePlugin', description='Summarize given text or any text document', name='Summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E8A4FEE0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E8A7BDC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Topics': KernelFunction(plugin_name='SummarizePlugin', description='Analyze given text or document and extract key topics worth remembering', name='Topics', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B280>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B1F0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 128, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=128, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'Acronym': KernelFunction(plugin_name='WriterPlugin', description='Generate an acronym for the given concept or phrase', name='Acronym', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B940>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9445A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymGenerator': KernelFunction(plugin_name='WriterPlugin', description='Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.', name='AcronymGenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B700>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.7, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymReverse': KernelFunction(plugin_name='WriterPlugin', description='Given a single word or acronym, generate the expanded form matching the acronym letters.', name='AcronymReverse', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BA60>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B9D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.8, 'frequency_penalty': 0.0, 'stop_sequences': ['#END#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.8, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Brainstorm': KernelFunction(plugin_name='WriterPlugin', description='Given a goal or topic description generate a list of ideas', name='Brainstorm', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BDC0>, parameters=[ParameterView(name='input', description='A topic description or goal.', default_value='', type_=None, required=False), ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['##END##']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Must: brainstorm ideas and create a list.\\nMust: use a numbered list.\\nMust: only one list.\\nMust: end list with ##END##\\nShould: no more than 10 items.\\nShould: at least 3 items.\\nTopic: Valentine's Day Date Ideas\\nStart.\\n\"}], extra_body=None), chat_prompt_template=None), 'EmailGen': KernelFunction(plugin_name='WriterPlugin', description='Write an email from the given bullet points', name='EmailGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BD30>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BC10>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EmailTo': KernelFunction(plugin_name='WriterPlugin', description='Turn bullet points into an email to someone, using a polite tone', name='EmailTo', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BEE0>, parameters=[ParameterView(name='to', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='sender', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BE50>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Rewrite my bullet points into an email featuring complete sentences. Use a polite and inclusive tone.  \\n\\n[Input]\\nToby,\\n\\n- Macbeth, King Scotland\\n- Married, Wife Lady Macbeth, No Kids\\n- Dog Toby McDuff. Hunter, dead. \\n- Shakespeare play\\n\\nThanks,\\nDexter\\n\\n+++++\\nHi Toby,\\n\\nThe story of Macbeth\\nMy name is Macbeth. I used to be King of Scotland, but I died. My wife's name is Lady Macbeth and we were married for 15 years. We had no children. Our beloved dog Toby McDuff was a famous hunter of rats in the forest.\\nMy story was immortalized by Shakespeare in a play.\\n\\nThanks,\\nDexter\\n\\n+++++\\n[Input]\\n\\n1. Picnic in the park\\n2. Cooking a romantic dinner together\\n3. Going to a wine tasting\\n4. Ice skating\\n5. Taking a hot air balloon ride\\n6. Watching a romantic movie at home\\n7. Going on a hike and having a picnic at the top\\n8. Taking a dance class together\\n9. Visiting a local museum or art gallery\\n10. Having a spa day together\\n##END##\\n\\nThanks,\\n\\n+++++\\n\"}], extra_body=None), chat_prompt_template=None), 'EnglishImprover': KernelFunction(plugin_name='WriterPlugin', description='Translate text to English and improve it', name='EnglishImprover', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BCA0>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BF70>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 3000, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=3000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapter': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel.', name='NovelChapter', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556040>, parameters=[ParameterView(name='input', description='A synopsis of what the chapter should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme or topic of this novel.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The synopsis of the previous chapter.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556160>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.3, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.3, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapterWithNotes': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel using notes about the chapter to write.', name='NovelChapterWithNotes', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95561F0>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme of this novel.', default_value='', type_=None, required=False), ParameterView(name='notes', description='Notes useful to write this chapter.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The previous chapter synopsis.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95560D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 1024, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelOutline': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of chapter synopsis for a novel or novella', name='NovelOutline', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512A60>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='chapterCount', description='The number of chapters to generate.', default_value='', type_=None, required=False), ParameterView(name='endMarker', description='The marker to use to end each chapter.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512EE0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.1, 'top_p': 0.5, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.1, top_p=0.5, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Rewrite': KernelFunction(plugin_name='WriterPlugin', description='Automatically generate compact notes for any text or text document', name='Rewrite', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512B80>, parameters=[ParameterView(name='style', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512DC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'ShortPoem': KernelFunction(plugin_name='WriterPlugin', description='Turn a scenario into a short and entertaining poem.', name='ShortPoem', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512AF0>, parameters=[ParameterView(name='input', description='The scenario to turn into a poem.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95129D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 60, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=60, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'StoryGen': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of synopsis for a novel or novella with sub-chapters', name='StoryGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556430>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556280>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 250, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=250, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'TellMeMore': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='TellMeMore', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95564C0>, parameters=[ParameterView(name='conversationtype', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='focusarea', description='', default_value='', type_='string', required=False), ParameterView(name='previousresults', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556310>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 500, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=500, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Translate': KernelFunction(plugin_name='WriterPlugin', description='Translate the input into a language of your choice', name='Translate', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556670>, parameters=[ParameterView(name='input', description='Text to translate', default_value='', type_=None, required=False), ParameterView(name='language', description='Language to translate to', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95565E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.7, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['[done]']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Translate the input below into English\\n\\nMAKE SURE YOU ONLY USE English.\\n\\nHi there,\\n\\nI hope this email finds you well. I wanted to share some ideas for fun and romantic activities that we could do together:\\n\\n1. We could have a lovely picnic in the park, surrounded by nature and fresh air.\\n2. Cooking a romantic dinner together could be a fun and intimate way to spend an evening.\\n3. Going to a wine tasting would be a great opportunity to try new wines and learn more about them.\\n4. Ice skating is always a fun and playful activity that we could enjoy together.\\n5. Taking a hot air balloon ride would be a unique and exciting experience that we could share.\\n6. We could also have a cozy night in and watch a romantic movie together.\\n7. Going on a hike and having a picnic at the top would be a great way to enjoy the outdoors and each other's company.\\n8. Taking a dance class together could be a fun and romantic way to learn something new.\\n9. Visiting a local museum or art gallery would be a great way to appreciate art and culture together.\\n10. Finally, we could have a relaxing spa day together and enjoy some pampering and relaxation.\\n\\nLet me know what you think and if you have any other ideas!\\n\\nThanks,\\n[Your Name]\\n\\nTranslation:\\n\"}], extra_body=None), chat_prompt_template=None), 'TwoSentenceSummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in two sentences or less', name='TwoSentenceSummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95563A0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556790>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'TextPlugin': KernelPlugin(name='TextPlugin', description=None, functions={'lowercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_tSNsOTIMHWAhndYw': KernelPlugin(name='p_tSNsOTIMHWAhndYw', description=None, functions={'f_QOkQhOcvcIJkhbYg': KernelFunction(plugin_name='p_tSNsOTIMHWAhndYw', description='Generic function, unknown purpose', name='f_QOkQhOcvcIJkhbYg', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E964DF70>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B550>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\\nYou are a planner for the Semantic Kernel.\\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\\nCreate a list of subtasks based off the [GOAL] provided.\\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\\nBase your decisions on which functions to use from the description and the name of the function.\\nSometimes, a function may take arguments. Provide them if necessary.\\nThe plan should be as short as possible.\\nFor example:\\n\\n[AVAILABLE FUNCTIONS]\\nEmailConnector.LookupContactEmail\\ndescription: looks up the a contact and retrieves their email address\\nargs:\\n- name: the name to look up\\n\\nWriterPlugin.EmailTo\\ndescription: email the input text to a recipient\\nargs:\\n- input: the text to email\\n- recipient: the recipient\\'s email address. Multiple addresses may be included if separated by \\';\\'.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\nWriterPlugin.Summarize\\ndescription: summarize input text\\nargs:\\n- input: the text to summarize\\n\\nFunPlugin.Joke\\ndescription: Generate a funny joke\\nargs:\\n- input: the input to generate a joke about\\n\\n[GOAL]\\n\"Tell a joke about cars. Translate it to Spanish\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"cars\",\\n        \"subtasks\": [\\n            {\"function\": \"FunPlugin.Joke\"},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"Spanish\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nWriterPlugin.Brainstorm\\ndescription: Brainstorm ideas\\nargs:\\n- input: the input to brainstorm about\\n\\nEdgarAllenPoePlugin.Poe\\ndescription: Write in the style of author Edgar Allen Poe\\nargs:\\n- input: the input to write about\\n\\nWriterPlugin.EmailTo\\ndescription: Write an email to a recipient\\nargs:\\n- input: the input to write about\\n- recipient: the recipient\\'s email address.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\n[GOAL]\\n\"Tomorrow is Valentine\\'s day. I need to come up with a few date ideas.\\nShe likes Edgar Allen Poe so write using his style.\\nE-mail these ideas to my significant other. Translate it to French.\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"Valentine\\'s Day Date Ideas\",\\n        \"subtasks\": [\\n            {\"function\": \"WriterPlugin.Brainstorm\"},\\n            {\"function\": \"EdgarAllenPoePlugin.Poe\"},\\n            {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"French\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nTextPlugin.lowercase\\ndescription: Convert a string to lowercase.\\nargs:\\n\\nTextPlugin.trim\\ndescription: Trim whitespace from the start and end of a string.\\nargs:\\n\\nTextPlugin.trim_end\\ndescription: Trim whitespace from the end of a string.\\nargs:\\n\\nTextPlugin.trim_start\\ndescription: Trim whitespace from the start of a string.\\nargs:\\n\\nTextPlugin.uppercase\\ndescription: Convert a string to uppercase.\\nargs:\\n\\nSummarizePlugin.MakeAbstractReadable\\ndescription: Given a scientific white paper abstract, rewrite it to make it more readable\\nargs:\\n- input: \\n\\nSummarizePlugin.Notegen\\ndescription: Automatically generate compact notes for any text or text document.\\nargs:\\n- input: \\n\\nSummarizePlugin.Summarize\\ndescription: Summarize given text or any text document\\nargs:\\n- input: Text to summarize\\n\\nSummarizePlugin.Topics\\ndescription: Analyze given text or document and extract key topics worth remembering\\nargs:\\n- input: \\n\\nWriterPlugin.Acronym\\ndescription: Generate an acronym for the given concept or phrase\\nargs:\\n- input: \\n\\nWriterPlugin.AcronymGenerator\\ndescription: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.AcronymReverse\\ndescription: Given a single word or acronym, generate the expanded form matching the acronym letters.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.Brainstorm\\ndescription: Given a goal or topic description generate a list of ideas\\nargs:\\n- input: A topic description or goal.\\n- INPUT: \\n\\nWriterPlugin.EmailGen\\ndescription: Write an email from the given bullet points\\nargs:\\n- input: \\n\\nWriterPlugin.EmailTo\\ndescription: Turn bullet points into an email to someone, using a polite tone\\nargs:\\n- to: \\n- input: \\n- sender: \\n\\nWriterPlugin.EnglishImprover\\ndescription: Translate text to English and improve it\\nargs:\\n- INPUT: \\n\\nWriterPlugin.NovelChapter\\ndescription: Write a chapter of a novel.\\nargs:\\n- input: A synopsis of what the chapter should be about.\\n- theme: The theme or topic of this novel.\\n- previousChapter: The synopsis of the previous chapter.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelChapterWithNotes\\ndescription: Write a chapter of a novel using notes about the chapter to write.\\nargs:\\n- input: What the novel should be about.\\n- theme: The theme of this novel.\\n- notes: Notes useful to write this chapter.\\n- previousChapter: The previous chapter synopsis.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline\\ndescription: Generate a list of chapter synopsis for a novel or novella\\nargs:\\n- input: What the novel should be about.\\n- chapterCount: The number of chapters to generate.\\n- endMarker: The marker to use to end each chapter.\\n\\nWriterPlugin.Rewrite\\ndescription: Automatically generate compact notes for any text or text document\\nargs:\\n- style: \\n- input: \\n\\nWriterPlugin.ShortPoem\\ndescription: Turn a scenario into a short and entertaining poem.\\nargs:\\n- input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen\\ndescription: Generate a list of synopsis for a novel or novella with sub-chapters\\nargs:\\n- input: \\n\\nWriterPlugin.TellMeMore\\ndescription: Summarize given text or any text document\\nargs:\\n- conversationtype: \\n- input: \\n- focusarea: \\n- previousresults: \\n\\nWriterPlugin.Translate\\ndescription: Translate the input into a language of your choice\\nargs:\\n- input: Text to translate\\n- language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary\\ndescription: Summarize given text in two sentences or less\\nargs:\\n- input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\ndescription: Generic function, unknown purpose\\nargs:\\n- available_functions: \\n- goal: \\n\\n\\n\\n[GOAL]\\n\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n\\n\\n[OUTPUT]\\n'}], extra_body=None), chat_prompt_template=None)}), 'SequentialPlanner_Excluded': KernelPlugin(name='SequentialPlanner_Excluded', description=None, functions={'SequentialPlanner_Excluded': KernelFunction(plugin_name='SequentialPlanner_Excluded', description='Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow', name='SequentialPlanner_Excluded', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969B4C0>, parameters=[ParameterView(name='input', description='The question to answer', default_value='', type_=None, required=False), ParameterView(name='available_functions', description=\"The list of the agent's available_functions\", default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Create an XML plan step by step, to satisfy the goal given, with the available functions.\\n\\n[AVAILABLE FUNCTIONS]\\n\\nSummarizePlugin.MakeAbstractReadable:\\n  description: Given a scientific white paper abstract, rewrite it to make it more readable\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Notegen:\\n  description: Automatically generate compact notes for any text or text document.\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Summarize:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - input: Text to summarize\\n\\nSummarizePlugin.Topics:\\n  description: Analyze given text or document and extract key topics worth remembering\\n  inputs:\\n    - input: \\n\\nWriterPlugin.Acronym:\\n  description: Generate an acronym for the given concept or phrase\\n  inputs:\\n    - input: \\n\\nWriterPlugin.AcronymGenerator:\\n  description: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.AcronymReverse:\\n  description: Given a single word or acronym, generate the expanded form matching the acronym letters.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.Brainstorm:\\n  description: Given a goal or topic description generate a list of ideas\\n  inputs:\\n    - input: A topic description or goal.\\n  - INPUT: \\n\\nWriterPlugin.EmailGen:\\n  description: Write an email from the given bullet points\\n  inputs:\\n    - input: \\n\\nWriterPlugin.EmailTo:\\n  description: Turn bullet points into an email to someone, using a polite tone\\n  inputs:\\n    - to: \\n  - input: \\n  - sender: \\n\\nWriterPlugin.EnglishImprover:\\n  description: Translate text to English and improve it\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.NovelChapter:\\n  description: Write a chapter of a novel.\\n  inputs:\\n    - input: A synopsis of what the chapter should be about.\\n  - theme: The theme or topic of this novel.\\n  - previousChapter: The synopsis of the previous chapter.\\n  - chapterIndex: The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.NovelChapterWithNotes:\\n  description: Write a chapter of a novel using notes about the chapter to write.\\n  inputs:\\n    - input: What the novel should be about.\\n  - theme: The theme of this novel.\\n  - notes: Notes useful to write this chapter.\\n  - previousChapter: The previous chapter synopsis.\\n  - chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline:\\n  description: Generate a list of chapter synopsis for a novel or novella\\n  inputs:\\n    - input: What the novel should be about.\\n  - chapterCount: The number of chapters to generate.\\n  - endMarker: The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.Rewrite:\\n  description: Automatically generate compact notes for any text or text document\\n  inputs:\\n    - style: \\n  - input: \\n\\nWriterPlugin.ShortPoem:\\n  description: Turn a scenario into a short and entertaining poem.\\n  inputs:\\n    - input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen:\\n  description: Generate a list of synopsis for a novel or novella with sub-chapters\\n  inputs:\\n    - input: \\n\\nWriterPlugin.TellMeMore:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - conversationtype: \\n  - input: \\n  - focusarea: \\n  - previousresults: \\n\\nWriterPlugin.Translate:\\n  description: Translate the input into a language of your choice\\n  inputs:\\n    - input: Text to translate\\n  - language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary:\\n  description: Summarize given text in two sentences or less\\n  inputs:\\n    - input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg:\\n  description: Generic function, unknown purpose\\n  inputs:\\n    - available_functions: \\n  - goal: \\n\\nTextPlugin.lowercase:\\n  description: Convert a string to lowercase.\\n  inputs:\\n  \\n\\nTextPlugin.trim:\\n  description: Trim whitespace from the start and end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_end:\\n  description: Trim whitespace from the end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_start:\\n  description: Trim whitespace from the start of a string.\\n  inputs:\\n  \\n\\nTextPlugin.uppercase:\\n  description: Convert a string to uppercase.\\n  inputs:\\n  \\n\\n[END AVAILABLE FUNCTIONS]\\n\\nTo create a plan, follow these steps:\\n0. The plan should be as short as possible.\\n1. From a <goal> create a <plan> as a series of <functions>.\\n2. A plan has \\'INPUT\\' available in context variables by default.\\n3. Before using any function in a plan, check that it is present in the [AVAILABLE FUNCTIONS] list. If it is not, do not use it.\\n4. Only use functions that are required for the given goal.\\n5. Append an \"END\" XML comment at the end of the plan after the final closing </plan> tag.\\n6. Always output valid XML that can be parsed by an XML parser.\\n7. If a plan cannot be created with the [AVAILABLE FUNCTIONS], return <plan />.\\n\\nAll plans take the form of:\\n<plan>\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    (... etc ...)\\n</plan>\\n<!-- END -->\\n\\nTo call a function, follow these steps:\\n1. A function has one or more named parameters and a single \\'output\\' which are all strings. Parameter values should be xml escaped.\\n2. To save an \\'output\\' from a <function>, to pass into a future <function>, use <function.{FullyQualifiedFunctionName} ... setContextVariable=\"<UNIQUE_VARIABLE_KEY>\"/>\\n3. To save an \\'output\\' from a <function>, to return as part of a plan result, use <function.{FullyQualifiedFunctionName} ... appendToResult=\"RESULT__<UNIQUE_RESULT_KEY>\"/>\\n4. Use a \\'$\\' to reference a context variable in a parameter, e.g. when `INPUT=\\'world\\'` the parameter \\'Hello $INPUT\\' will evaluate to `Hello world`.\\n5. Functions do not have access to the context variables of other functions. Do not attempt to use context variables as arrays or objects. Instead, use available functions to extract specific elements or properties from context variables.\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS NOT XML ESCAPED:\\n<function.Name4 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a <!-- \\'comment\\' in it-->\"/>\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS ATTEMPTING TO USE A CONTEXT VARIABLE AS AN ARRAY/OBJECT:\\n<function.CallFunction input=\"$OTHER_OUTPUT[1]\"/>\\n\\nHere is a valid example of how to call a function \"_Function_.Name\" with a single input and save its output:\\n<function._Function_.Name input=\"this is my input\" setContextVariable=\"SOME_KEY\"/>\\n\\nHere is a valid example of how to call a function \"FunctionName2\" with a single input and return its output as part of the plan result:\\n<function.FunctionName2 input=\"Hello $INPUT\" appendToResult=\"RESULT__FINAL_ANSWER\"/>\\n\\nHere is a valid example of how to call a function \"Name3\" with multiple inputs:\\n<function.Name3 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a &lt;!-- &apos;comment&apos; in it--&gt;\"/>\\n\\nBegin!\\n\\n<goal>\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n</goal>\\n'}], extra_body=None), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_vvSKvltRilNtxOYw': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_vvSKvltRilNtxOYw', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969BF70>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'fileIO': KernelPlugin(name='fileIO', description=None, functions={'readAsync': KernelFunction(plugin_name='fileIO', description='Read a file', name='readAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.read of FileIOPlugin()>, parameters=[ParameterView(name='input', description='Path of the source file', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringOutTaskString: 10>, function=<bound method FileIOPlugin.read of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'writeAsync': KernelFunction(plugin_name='fileIO', description='Write a file', name='writeAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.write of FileIOPlugin()>, parameters=[ParameterView(name='content', description='File content', default_value='', type_='string', required=False), ParameterView(name='path', description='Destination path', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InContextOutTask: 16>, function=<bound method FileIOPlugin.write of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'time': KernelPlugin(name='time', description=None, functions={'date': KernelFunction(plugin_name='time', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='time', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='time', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='time', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='time', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='time', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='time', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='time', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='time', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='time', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='time', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='time', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='time', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='time', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='time', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='time', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='time', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='time', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='time', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'text': KernelPlugin(name='text', description=None, functions={...})}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='text', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={'SummarizePlugin': KernelPlugin(name='SummarizePlugin', description=None, functions={'MakeAbstractReadable': KernelFunction(plugin_name='SummarizePlugin', description='Given a scientific white paper abstract, rewrite it to make it more readable', name='MakeAbstractReadable', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7C8E19DC0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 2.0}, ai_model_id=None, frequency_penalty=2.0, logit_bias={}, max_tokens=4000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Notegen': KernelFunction(plugin_name='SummarizePlugin', description='Automatically generate compact notes for any text or text document.', name='Notegen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B0D0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4AF0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Summarize': KernelFunction(plugin_name='SummarizePlugin', description='Summarize given text or any text document', name='Summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E8A4FEE0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E8A7BDC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Topics': KernelFunction(plugin_name='SummarizePlugin', description='Analyze given text or document and extract key topics worth remembering', name='Topics', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B280>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B1F0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 128, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=128, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'Acronym': KernelFunction(plugin_name='WriterPlugin', description='Generate an acronym for the given concept or phrase', name='Acronym', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B940>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9445A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymGenerator': KernelFunction(plugin_name='WriterPlugin', description='Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.', name='AcronymGenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B700>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.7, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymReverse': KernelFunction(plugin_name='WriterPlugin', description='Given a single word or acronym, generate the expanded form matching the acronym letters.', name='AcronymReverse', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BA60>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B9D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.8, 'frequency_penalty': 0.0, 'stop_sequences': ['#END#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.8, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Brainstorm': KernelFunction(plugin_name='WriterPlugin', description='Given a goal or topic description generate a list of ideas', name='Brainstorm', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BDC0>, parameters=[ParameterView(name='input', description='A topic description or goal.', default_value='', type_=None, required=False), ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['##END##']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Must: brainstorm ideas and create a list.\\nMust: use a numbered list.\\nMust: only one list.\\nMust: end list with ##END##\\nShould: no more than 10 items.\\nShould: at least 3 items.\\nTopic: Valentine's Day Date Ideas\\nStart.\\n\"}], extra_body=None), chat_prompt_template=None), 'EmailGen': KernelFunction(plugin_name='WriterPlugin', description='Write an email from the given bullet points', name='EmailGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BD30>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BC10>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EmailTo': KernelFunction(plugin_name='WriterPlugin', description='Turn bullet points into an email to someone, using a polite tone', name='EmailTo', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BEE0>, parameters=[ParameterView(name='to', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='sender', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BE50>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Rewrite my bullet points into an email featuring complete sentences. Use a polite and inclusive tone.  \\n\\n[Input]\\nToby,\\n\\n- Macbeth, King Scotland\\n- Married, Wife Lady Macbeth, No Kids\\n- Dog Toby McDuff. Hunter, dead. \\n- Shakespeare play\\n\\nThanks,\\nDexter\\n\\n+++++\\nHi Toby,\\n\\nThe story of Macbeth\\nMy name is Macbeth. I used to be King of Scotland, but I died. My wife's name is Lady Macbeth and we were married for 15 years. We had no children. Our beloved dog Toby McDuff was a famous hunter of rats in the forest.\\nMy story was immortalized by Shakespeare in a play.\\n\\nThanks,\\nDexter\\n\\n+++++\\n[Input]\\n\\n1. Picnic in the park\\n2. Cooking a romantic dinner together\\n3. Going to a wine tasting\\n4. Ice skating\\n5. Taking a hot air balloon ride\\n6. Watching a romantic movie at home\\n7. Going on a hike and having a picnic at the top\\n8. Taking a dance class together\\n9. Visiting a local museum or art gallery\\n10. Having a spa day together\\n##END##\\n\\nThanks,\\n\\n+++++\\n\"}], extra_body=None), chat_prompt_template=None), 'EnglishImprover': KernelFunction(plugin_name='WriterPlugin', description='Translate text to English and improve it', name='EnglishImprover', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BCA0>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BF70>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 3000, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=3000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapter': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel.', name='NovelChapter', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556040>, parameters=[ParameterView(name='input', description='A synopsis of what the chapter should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme or topic of this novel.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The synopsis of the previous chapter.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556160>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.3, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.3, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapterWithNotes': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel using notes about the chapter to write.', name='NovelChapterWithNotes', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95561F0>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme of this novel.', default_value='', type_=None, required=False), ParameterView(name='notes', description='Notes useful to write this chapter.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The previous chapter synopsis.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95560D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 1024, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelOutline': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of chapter synopsis for a novel or novella', name='NovelOutline', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512A60>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='chapterCount', description='The number of chapters to generate.', default_value='', type_=None, required=False), ParameterView(name='endMarker', description='The marker to use to end each chapter.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512EE0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.1, 'top_p': 0.5, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.1, top_p=0.5, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Rewrite': KernelFunction(plugin_name='WriterPlugin', description='Automatically generate compact notes for any text or text document', name='Rewrite', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512B80>, parameters=[ParameterView(name='style', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512DC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'ShortPoem': KernelFunction(plugin_name='WriterPlugin', description='Turn a scenario into a short and entertaining poem.', name='ShortPoem', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512AF0>, parameters=[ParameterView(name='input', description='The scenario to turn into a poem.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95129D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 60, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=60, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'StoryGen': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of synopsis for a novel or novella with sub-chapters', name='StoryGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556430>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556280>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 250, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=250, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'TellMeMore': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='TellMeMore', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95564C0>, parameters=[ParameterView(name='conversationtype', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='focusarea', description='', default_value='', type_='string', required=False), ParameterView(name='previousresults', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556310>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 500, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=500, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Translate': KernelFunction(plugin_name='WriterPlugin', description='Translate the input into a language of your choice', name='Translate', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556670>, parameters=[ParameterView(name='input', description='Text to translate', default_value='', type_=None, required=False), ParameterView(name='language', description='Language to translate to', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95565E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.7, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['[done]']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Translate the input below into English\\n\\nMAKE SURE YOU ONLY USE English.\\n\\nHi there,\\n\\nI hope this email finds you well. I wanted to share some ideas for fun and romantic activities that we could do together:\\n\\n1. We could have a lovely picnic in the park, surrounded by nature and fresh air.\\n2. Cooking a romantic dinner together could be a fun and intimate way to spend an evening.\\n3. Going to a wine tasting would be a great opportunity to try new wines and learn more about them.\\n4. Ice skating is always a fun and playful activity that we could enjoy together.\\n5. Taking a hot air balloon ride would be a unique and exciting experience that we could share.\\n6. We could also have a cozy night in and watch a romantic movie together.\\n7. Going on a hike and having a picnic at the top would be a great way to enjoy the outdoors and each other's company.\\n8. Taking a dance class together could be a fun and romantic way to learn something new.\\n9. Visiting a local museum or art gallery would be a great way to appreciate art and culture together.\\n10. Finally, we could have a relaxing spa day together and enjoy some pampering and relaxation.\\n\\nLet me know what you think and if you have any other ideas!\\n\\nThanks,\\n[Your Name]\\n\\nTranslation:\\n\"}], extra_body=None), chat_prompt_template=None), 'TwoSentenceSummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in two sentences or less', name='TwoSentenceSummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95563A0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556790>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'TextPlugin': KernelPlugin(name='TextPlugin', description=None, functions={'lowercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_tSNsOTIMHWAhndYw': KernelPlugin(name='p_tSNsOTIMHWAhndYw', description=None, functions={'f_QOkQhOcvcIJkhbYg': KernelFunction(plugin_name='p_tSNsOTIMHWAhndYw', description='Generic function, unknown purpose', name='f_QOkQhOcvcIJkhbYg', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E964DF70>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B550>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\\nYou are a planner for the Semantic Kernel.\\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\\nCreate a list of subtasks based off the [GOAL] provided.\\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\\nBase your decisions on which functions to use from the description and the name of the function.\\nSometimes, a function may take arguments. Provide them if necessary.\\nThe plan should be as short as possible.\\nFor example:\\n\\n[AVAILABLE FUNCTIONS]\\nEmailConnector.LookupContactEmail\\ndescription: looks up the a contact and retrieves their email address\\nargs:\\n- name: the name to look up\\n\\nWriterPlugin.EmailTo\\ndescription: email the input text to a recipient\\nargs:\\n- input: the text to email\\n- recipient: the recipient\\'s email address. Multiple addresses may be included if separated by \\';\\'.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\nWriterPlugin.Summarize\\ndescription: summarize input text\\nargs:\\n- input: the text to summarize\\n\\nFunPlugin.Joke\\ndescription: Generate a funny joke\\nargs:\\n- input: the input to generate a joke about\\n\\n[GOAL]\\n\"Tell a joke about cars. Translate it to Spanish\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"cars\",\\n        \"subtasks\": [\\n            {\"function\": \"FunPlugin.Joke\"},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"Spanish\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nWriterPlugin.Brainstorm\\ndescription: Brainstorm ideas\\nargs:\\n- input: the input to brainstorm about\\n\\nEdgarAllenPoePlugin.Poe\\ndescription: Write in the style of author Edgar Allen Poe\\nargs:\\n- input: the input to write about\\n\\nWriterPlugin.EmailTo\\ndescription: Write an email to a recipient\\nargs:\\n- input: the input to write about\\n- recipient: the recipient\\'s email address.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\n[GOAL]\\n\"Tomorrow is Valentine\\'s day. I need to come up with a few date ideas.\\nShe likes Edgar Allen Poe so write using his style.\\nE-mail these ideas to my significant other. Translate it to French.\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"Valentine\\'s Day Date Ideas\",\\n        \"subtasks\": [\\n            {\"function\": \"WriterPlugin.Brainstorm\"},\\n            {\"function\": \"EdgarAllenPoePlugin.Poe\"},\\n            {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"French\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nTextPlugin.lowercase\\ndescription: Convert a string to lowercase.\\nargs:\\n\\nTextPlugin.trim\\ndescription: Trim whitespace from the start and end of a string.\\nargs:\\n\\nTextPlugin.trim_end\\ndescription: Trim whitespace from the end of a string.\\nargs:\\n\\nTextPlugin.trim_start\\ndescription: Trim whitespace from the start of a string.\\nargs:\\n\\nTextPlugin.uppercase\\ndescription: Convert a string to uppercase.\\nargs:\\n\\nSummarizePlugin.MakeAbstractReadable\\ndescription: Given a scientific white paper abstract, rewrite it to make it more readable\\nargs:\\n- input: \\n\\nSummarizePlugin.Notegen\\ndescription: Automatically generate compact notes for any text or text document.\\nargs:\\n- input: \\n\\nSummarizePlugin.Summarize\\ndescription: Summarize given text or any text document\\nargs:\\n- input: Text to summarize\\n\\nSummarizePlugin.Topics\\ndescription: Analyze given text or document and extract key topics worth remembering\\nargs:\\n- input: \\n\\nWriterPlugin.Acronym\\ndescription: Generate an acronym for the given concept or phrase\\nargs:\\n- input: \\n\\nWriterPlugin.AcronymGenerator\\ndescription: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.AcronymReverse\\ndescription: Given a single word or acronym, generate the expanded form matching the acronym letters.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.Brainstorm\\ndescription: Given a goal or topic description generate a list of ideas\\nargs:\\n- input: A topic description or goal.\\n- INPUT: \\n\\nWriterPlugin.EmailGen\\ndescription: Write an email from the given bullet points\\nargs:\\n- input: \\n\\nWriterPlugin.EmailTo\\ndescription: Turn bullet points into an email to someone, using a polite tone\\nargs:\\n- to: \\n- input: \\n- sender: \\n\\nWriterPlugin.EnglishImprover\\ndescription: Translate text to English and improve it\\nargs:\\n- INPUT: \\n\\nWriterPlugin.NovelChapter\\ndescription: Write a chapter of a novel.\\nargs:\\n- input: A synopsis of what the chapter should be about.\\n- theme: The theme or topic of this novel.\\n- previousChapter: The synopsis of the previous chapter.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelChapterWithNotes\\ndescription: Write a chapter of a novel using notes about the chapter to write.\\nargs:\\n- input: What the novel should be about.\\n- theme: The theme of this novel.\\n- notes: Notes useful to write this chapter.\\n- previousChapter: The previous chapter synopsis.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline\\ndescription: Generate a list of chapter synopsis for a novel or novella\\nargs:\\n- input: What the novel should be about.\\n- chapterCount: The number of chapters to generate.\\n- endMarker: The marker to use to end each chapter.\\n\\nWriterPlugin.Rewrite\\ndescription: Automatically generate compact notes for any text or text document\\nargs:\\n- style: \\n- input: \\n\\nWriterPlugin.ShortPoem\\ndescription: Turn a scenario into a short and entertaining poem.\\nargs:\\n- input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen\\ndescription: Generate a list of synopsis for a novel or novella with sub-chapters\\nargs:\\n- input: \\n\\nWriterPlugin.TellMeMore\\ndescription: Summarize given text or any text document\\nargs:\\n- conversationtype: \\n- input: \\n- focusarea: \\n- previousresults: \\n\\nWriterPlugin.Translate\\ndescription: Translate the input into a language of your choice\\nargs:\\n- input: Text to translate\\n- language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary\\ndescription: Summarize given text in two sentences or less\\nargs:\\n- input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\ndescription: Generic function, unknown purpose\\nargs:\\n- available_functions: \\n- goal: \\n\\n\\n\\n[GOAL]\\n\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n\\n\\n[OUTPUT]\\n'}], extra_body=None), chat_prompt_template=None)}), 'SequentialPlanner_Excluded': KernelPlugin(name='SequentialPlanner_Excluded', description=None, functions={'SequentialPlanner_Excluded': KernelFunction(plugin_name='SequentialPlanner_Excluded', description='Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow', name='SequentialPlanner_Excluded', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969B4C0>, parameters=[ParameterView(name='input', description='The question to answer', default_value='', type_=None, required=False), ParameterView(name='available_functions', description=\"The list of the agent's available_functions\", default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Create an XML plan step by step, to satisfy the goal given, with the available functions.\\n\\n[AVAILABLE FUNCTIONS]\\n\\nSummarizePlugin.MakeAbstractReadable:\\n  description: Given a scientific white paper abstract, rewrite it to make it more readable\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Notegen:\\n  description: Automatically generate compact notes for any text or text document.\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Summarize:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - input: Text to summarize\\n\\nSummarizePlugin.Topics:\\n  description: Analyze given text or document and extract key topics worth remembering\\n  inputs:\\n    - input: \\n\\nWriterPlugin.Acronym:\\n  description: Generate an acronym for the given concept or phrase\\n  inputs:\\n    - input: \\n\\nWriterPlugin.AcronymGenerator:\\n  description: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.AcronymReverse:\\n  description: Given a single word or acronym, generate the expanded form matching the acronym letters.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.Brainstorm:\\n  description: Given a goal or topic description generate a list of ideas\\n  inputs:\\n    - input: A topic description or goal.\\n  - INPUT: \\n\\nWriterPlugin.EmailGen:\\n  description: Write an email from the given bullet points\\n  inputs:\\n    - input: \\n\\nWriterPlugin.EmailTo:\\n  description: Turn bullet points into an email to someone, using a polite tone\\n  inputs:\\n    - to: \\n  - input: \\n  - sender: \\n\\nWriterPlugin.EnglishImprover:\\n  description: Translate text to English and improve it\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.NovelChapter:\\n  description: Write a chapter of a novel.\\n  inputs:\\n    - input: A synopsis of what the chapter should be about.\\n  - theme: The theme or topic of this novel.\\n  - previousChapter: The synopsis of the previous chapter.\\n  - chapterIndex: The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.NovelChapterWithNotes:\\n  description: Write a chapter of a novel using notes about the chapter to write.\\n  inputs:\\n    - input: What the novel should be about.\\n  - theme: The theme of this novel.\\n  - notes: Notes useful to write this chapter.\\n  - previousChapter: The previous chapter synopsis.\\n  - chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline:\\n  description: Generate a list of chapter synopsis for a novel or novella\\n  inputs:\\n    - input: What the novel should be about.\\n  - chapterCount: The number of chapters to generate.\\n  - endMarker: The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.Rewrite:\\n  description: Automatically generate compact notes for any text or text document\\n  inputs:\\n    - style: \\n  - input: \\n\\nWriterPlugin.ShortPoem:\\n  description: Turn a scenario into a short and entertaining poem.\\n  inputs:\\n    - input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen:\\n  description: Generate a list of synopsis for a novel or novella with sub-chapters\\n  inputs:\\n    - input: \\n\\nWriterPlugin.TellMeMore:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - conversationtype: \\n  - input: \\n  - focusarea: \\n  - previousresults: \\n\\nWriterPlugin.Translate:\\n  description: Translate the input into a language of your choice\\n  inputs:\\n    - input: Text to translate\\n  - language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary:\\n  description: Summarize given text in two sentences or less\\n  inputs:\\n    - input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg:\\n  description: Generic function, unknown purpose\\n  inputs:\\n    - available_functions: \\n  - goal: \\n\\nTextPlugin.lowercase:\\n  description: Convert a string to lowercase.\\n  inputs:\\n  \\n\\nTextPlugin.trim:\\n  description: Trim whitespace from the start and end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_end:\\n  description: Trim whitespace from the end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_start:\\n  description: Trim whitespace from the start of a string.\\n  inputs:\\n  \\n\\nTextPlugin.uppercase:\\n  description: Convert a string to uppercase.\\n  inputs:\\n  \\n\\n[END AVAILABLE FUNCTIONS]\\n\\nTo create a plan, follow these steps:\\n0. The plan should be as short as possible.\\n1. From a <goal> create a <plan> as a series of <functions>.\\n2. A plan has \\'INPUT\\' available in context variables by default.\\n3. Before using any function in a plan, check that it is present in the [AVAILABLE FUNCTIONS] list. If it is not, do not use it.\\n4. Only use functions that are required for the given goal.\\n5. Append an \"END\" XML comment at the end of the plan after the final closing </plan> tag.\\n6. Always output valid XML that can be parsed by an XML parser.\\n7. If a plan cannot be created with the [AVAILABLE FUNCTIONS], return <plan />.\\n\\nAll plans take the form of:\\n<plan>\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    (... etc ...)\\n</plan>\\n<!-- END -->\\n\\nTo call a function, follow these steps:\\n1. A function has one or more named parameters and a single \\'output\\' which are all strings. Parameter values should be xml escaped.\\n2. To save an \\'output\\' from a <function>, to pass into a future <function>, use <function.{FullyQualifiedFunctionName} ... setContextVariable=\"<UNIQUE_VARIABLE_KEY>\"/>\\n3. To save an \\'output\\' from a <function>, to return as part of a plan result, use <function.{FullyQualifiedFunctionName} ... appendToResult=\"RESULT__<UNIQUE_RESULT_KEY>\"/>\\n4. Use a \\'$\\' to reference a context variable in a parameter, e.g. when `INPUT=\\'world\\'` the parameter \\'Hello $INPUT\\' will evaluate to `Hello world`.\\n5. Functions do not have access to the context variables of other functions. Do not attempt to use context variables as arrays or objects. Instead, use available functions to extract specific elements or properties from context variables.\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS NOT XML ESCAPED:\\n<function.Name4 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a <!-- \\'comment\\' in it-->\"/>\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS ATTEMPTING TO USE A CONTEXT VARIABLE AS AN ARRAY/OBJECT:\\n<function.CallFunction input=\"$OTHER_OUTPUT[1]\"/>\\n\\nHere is a valid example of how to call a function \"_Function_.Name\" with a single input and save its output:\\n<function._Function_.Name input=\"this is my input\" setContextVariable=\"SOME_KEY\"/>\\n\\nHere is a valid example of how to call a function \"FunctionName2\" with a single input and return its output as part of the plan result:\\n<function.FunctionName2 input=\"Hello $INPUT\" appendToResult=\"RESULT__FINAL_ANSWER\"/>\\n\\nHere is a valid example of how to call a function \"Name3\" with multiple inputs:\\n<function.Name3 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a &lt;!-- &apos;comment&apos; in it--&gt;\"/>\\n\\nBegin!\\n\\n<goal>\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n</goal>\\n'}], extra_body=None), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_vvSKvltRilNtxOYw': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_vvSKvltRilNtxOYw', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969BF70>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=3781, completion_tokens=716, total_tokens=4497), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'fileIO': KernelPlugin(name='fileIO', description=None, functions={'readAsync': KernelFunction(plugin_name='fileIO', description='Read a file', name='readAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.read of FileIOPlugin()>, parameters=[ParameterView(name='input', description='Path of the source file', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringOutTaskString: 10>, function=<bound method FileIOPlugin.read of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'writeAsync': KernelFunction(plugin_name='fileIO', description='Write a file', name='writeAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.write of FileIOPlugin()>, parameters=[ParameterView(name='content', description='File content', default_value='', type_='string', required=False), ParameterView(name='path', description='Destination path', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InContextOutTask: 16>, function=<bound method FileIOPlugin.write of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'time': KernelPlugin(name='time', description=None, functions={'date': KernelFunction(plugin_name='time', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='time', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='time', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='time', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='time', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='time', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='time', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='time', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='time', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='time', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='time', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='time', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='time', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='time', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='time', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='time', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='time', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='time', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='time', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'text': KernelPlugin(name='text', description=None, functions={...})}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel.core_plugins import (\n",
    "    FileIOPlugin,\n",
    "    MathPlugin,\n",
    "    TextPlugin,\n",
    "    TimePlugin\n",
    ")\n",
    "\n",
    "kernel.import_plugin(MathPlugin(), \"math\")\n",
    "kernel.import_plugin(FileIOPlugin(), \"fileIO\")\n",
    "kernel.import_plugin(TimePlugin(), \"time\")\n",
    "kernel.import_plugin(TextPlugin(), \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"what is the sum of 22 and 22?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = await planner.create_plan(goal=ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await plan.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stepwise Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepwise Planner is based off the paper from MRKL (Modular Reasoning, Knowledge and Language) and is similar to other papers like ReACT (Reasoning and Acting in Language Models). At the core, the stepwise planner allows for the AI to form \"thoughts\" and \"observations\" and execute actions based off those to achieve a user's goal. This continues until all required functions are complete and a final output is generated.\n",
    "\n",
    "See a video walkthrough of Stepwise Planner here.\n",
    "https://youtu.be/DG_Ge1v0c4Q?si=T1CHaAm1vV0mWRHu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planning.stepwise_planner import StepwisePlanner\n",
    "from semantic_kernel.planning.stepwise_planner.stepwise_planner_config import StepwisePlannerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Bing Search native plugin that we can pass in to the Kernel.\n",
    "\n",
    "Make sure you have a Bing Search API key in your .env file\n",
    "\n",
    "(https://www.microsoft.com/en-us/bing/apis/bing-web-search-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchEnginePlugin:\n",
    "    \"\"\" A search engine plugin. \"\"\"\n",
    "\n",
    "    from semantic_kernel.orchestration.kernel_context import KernelContext\n",
    "    from semantic_kernel.plugin_definition import kernel_function, kernel_function_context_parameter\n",
    "\n",
    "    def __init__(self, connector) -> None:\n",
    "        self._connector = connector\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Performs a web search for a given query\",\n",
    "        name = \"searchAsync\"\n",
    "    )\n",
    "    @kernel_function_context_parameter(\n",
    "        name=\"query\",\n",
    "        description=\"The search query\"\n",
    "    )\n",
    "    async def search(self, query: str, context: KernelContext) -> str:\n",
    "        query = query or context.variables.get(\"query\")[1]\n",
    "        result = await self._connector.search(query, num_results=5, offset=0)\n",
    "        return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bing API key cannot be null. Please set environment variable BING_API_KEY.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BingConnector\n\u001b[0;32m      3\u001b[0m BING_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBING_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m connector \u001b[38;5;241m=\u001b[39m \u001b[43mBingConnector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBING_API_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m kernel\u001b[38;5;241m.\u001b[39mimport_plugin(WebSearchEnginePlugin(connector), plugin_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWebSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\2024_GenAI\\SampleProjects\\3_SK_Doc_Exp\\skdoc\\lib\\site-packages\\semantic_kernel\\connectors\\search_engine\\bing_connector.py:27\u001b[0m, in \u001b[0;36mBingConnector.__init__\u001b[1;34m(self, api_key, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBing API key cannot be null. Please set environment variable BING_API_KEY.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Bing API key cannot be null. Please set environment variable BING_API_KEY."
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.search_engine import BingConnector\n",
    "\n",
    "BING_API_KEY = os.getenv(\"BING_API_KEY\")\n",
    "connector = BingConnector(BING_API_KEY)\n",
    "kernel.import_plugin(WebSearchEnginePlugin(connector), plugin_name=\"WebSearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='mathh', description=None, functions={'Add': KernelFunction(plugin_name='mathh', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={'SummarizePlugin': KernelPlugin(name='SummarizePlugin', description=None, functions={'MakeAbstractReadable': KernelFunction(plugin_name='SummarizePlugin', description='Given a scientific white paper abstract, rewrite it to make it more readable', name='MakeAbstractReadable', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7C8E19DC0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 2.0}, ai_model_id=None, frequency_penalty=2.0, logit_bias={}, max_tokens=4000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Notegen': KernelFunction(plugin_name='SummarizePlugin', description='Automatically generate compact notes for any text or text document.', name='Notegen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B0D0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4AF0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Summarize': KernelFunction(plugin_name='SummarizePlugin', description='Summarize given text or any text document', name='Summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E8A4FEE0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E8A7BDC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Topics': KernelFunction(plugin_name='SummarizePlugin', description='Analyze given text or document and extract key topics worth remembering', name='Topics', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B280>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B1F0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 128, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=128, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'Acronym': KernelFunction(plugin_name='WriterPlugin', description='Generate an acronym for the given concept or phrase', name='Acronym', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B940>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9445A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymGenerator': KernelFunction(plugin_name='WriterPlugin', description='Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.', name='AcronymGenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B700>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.7, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymReverse': KernelFunction(plugin_name='WriterPlugin', description='Given a single word or acronym, generate the expanded form matching the acronym letters.', name='AcronymReverse', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BA60>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B9D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.8, 'frequency_penalty': 0.0, 'stop_sequences': ['#END#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.8, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Brainstorm': KernelFunction(plugin_name='WriterPlugin', description='Given a goal or topic description generate a list of ideas', name='Brainstorm', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BDC0>, parameters=[ParameterView(name='input', description='A topic description or goal.', default_value='', type_=None, required=False), ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['##END##']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Must: brainstorm ideas and create a list.\\nMust: use a numbered list.\\nMust: only one list.\\nMust: end list with ##END##\\nShould: no more than 10 items.\\nShould: at least 3 items.\\nTopic: Valentine's Day Date Ideas\\nStart.\\n\"}], extra_body=None), chat_prompt_template=None), 'EmailGen': KernelFunction(plugin_name='WriterPlugin', description='Write an email from the given bullet points', name='EmailGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BD30>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BC10>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EmailTo': KernelFunction(plugin_name='WriterPlugin', description='Turn bullet points into an email to someone, using a polite tone', name='EmailTo', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BEE0>, parameters=[ParameterView(name='to', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='sender', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BE50>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Rewrite my bullet points into an email featuring complete sentences. Use a polite and inclusive tone.  \\n\\n[Input]\\nToby,\\n\\n- Macbeth, King Scotland\\n- Married, Wife Lady Macbeth, No Kids\\n- Dog Toby McDuff. Hunter, dead. \\n- Shakespeare play\\n\\nThanks,\\nDexter\\n\\n+++++\\nHi Toby,\\n\\nThe story of Macbeth\\nMy name is Macbeth. I used to be King of Scotland, but I died. My wife's name is Lady Macbeth and we were married for 15 years. We had no children. Our beloved dog Toby McDuff was a famous hunter of rats in the forest.\\nMy story was immortalized by Shakespeare in a play.\\n\\nThanks,\\nDexter\\n\\n+++++\\n[Input]\\n\\n1. Picnic in the park\\n2. Cooking a romantic dinner together\\n3. Going to a wine tasting\\n4. Ice skating\\n5. Taking a hot air balloon ride\\n6. Watching a romantic movie at home\\n7. Going on a hike and having a picnic at the top\\n8. Taking a dance class together\\n9. Visiting a local museum or art gallery\\n10. Having a spa day together\\n##END##\\n\\nThanks,\\n\\n+++++\\n\"}], extra_body=None), chat_prompt_template=None), 'EnglishImprover': KernelFunction(plugin_name='WriterPlugin', description='Translate text to English and improve it', name='EnglishImprover', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BCA0>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BF70>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 3000, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=3000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapter': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel.', name='NovelChapter', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556040>, parameters=[ParameterView(name='input', description='A synopsis of what the chapter should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme or topic of this novel.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The synopsis of the previous chapter.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556160>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.3, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.3, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapterWithNotes': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel using notes about the chapter to write.', name='NovelChapterWithNotes', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95561F0>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme of this novel.', default_value='', type_=None, required=False), ParameterView(name='notes', description='Notes useful to write this chapter.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The previous chapter synopsis.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95560D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 1024, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelOutline': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of chapter synopsis for a novel or novella', name='NovelOutline', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512A60>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='chapterCount', description='The number of chapters to generate.', default_value='', type_=None, required=False), ParameterView(name='endMarker', description='The marker to use to end each chapter.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512EE0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.1, 'top_p': 0.5, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.1, top_p=0.5, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Rewrite': KernelFunction(plugin_name='WriterPlugin', description='Automatically generate compact notes for any text or text document', name='Rewrite', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512B80>, parameters=[ParameterView(name='style', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512DC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'ShortPoem': KernelFunction(plugin_name='WriterPlugin', description='Turn a scenario into a short and entertaining poem.', name='ShortPoem', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512AF0>, parameters=[ParameterView(name='input', description='The scenario to turn into a poem.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95129D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 60, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=60, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'StoryGen': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of synopsis for a novel or novella with sub-chapters', name='StoryGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556430>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556280>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 250, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=250, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'TellMeMore': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='TellMeMore', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95564C0>, parameters=[ParameterView(name='conversationtype', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='focusarea', description='', default_value='', type_='string', required=False), ParameterView(name='previousresults', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556310>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 500, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=500, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Translate': KernelFunction(plugin_name='WriterPlugin', description='Translate the input into a language of your choice', name='Translate', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556670>, parameters=[ParameterView(name='input', description='Text to translate', default_value='', type_=None, required=False), ParameterView(name='language', description='Language to translate to', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95565E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.7, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['[done]']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Translate the input below into English\\n\\nMAKE SURE YOU ONLY USE English.\\n\\nHi there,\\n\\nI hope this email finds you well. I wanted to share some ideas for fun and romantic activities that we could do together:\\n\\n1. We could have a lovely picnic in the park, surrounded by nature and fresh air.\\n2. Cooking a romantic dinner together could be a fun and intimate way to spend an evening.\\n3. Going to a wine tasting would be a great opportunity to try new wines and learn more about them.\\n4. Ice skating is always a fun and playful activity that we could enjoy together.\\n5. Taking a hot air balloon ride would be a unique and exciting experience that we could share.\\n6. We could also have a cozy night in and watch a romantic movie together.\\n7. Going on a hike and having a picnic at the top would be a great way to enjoy the outdoors and each other's company.\\n8. Taking a dance class together could be a fun and romantic way to learn something new.\\n9. Visiting a local museum or art gallery would be a great way to appreciate art and culture together.\\n10. Finally, we could have a relaxing spa day together and enjoy some pampering and relaxation.\\n\\nLet me know what you think and if you have any other ideas!\\n\\nThanks,\\n[Your Name]\\n\\nTranslation:\\n\"}], extra_body=None), chat_prompt_template=None), 'TwoSentenceSummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in two sentences or less', name='TwoSentenceSummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95563A0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556790>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'TextPlugin': KernelPlugin(name='TextPlugin', description=None, functions={'lowercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_tSNsOTIMHWAhndYw': KernelPlugin(name='p_tSNsOTIMHWAhndYw', description=None, functions={'f_QOkQhOcvcIJkhbYg': KernelFunction(plugin_name='p_tSNsOTIMHWAhndYw', description='Generic function, unknown purpose', name='f_QOkQhOcvcIJkhbYg', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E964DF70>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B550>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\\nYou are a planner for the Semantic Kernel.\\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\\nCreate a list of subtasks based off the [GOAL] provided.\\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\\nBase your decisions on which functions to use from the description and the name of the function.\\nSometimes, a function may take arguments. Provide them if necessary.\\nThe plan should be as short as possible.\\nFor example:\\n\\n[AVAILABLE FUNCTIONS]\\nEmailConnector.LookupContactEmail\\ndescription: looks up the a contact and retrieves their email address\\nargs:\\n- name: the name to look up\\n\\nWriterPlugin.EmailTo\\ndescription: email the input text to a recipient\\nargs:\\n- input: the text to email\\n- recipient: the recipient\\'s email address. Multiple addresses may be included if separated by \\';\\'.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\nWriterPlugin.Summarize\\ndescription: summarize input text\\nargs:\\n- input: the text to summarize\\n\\nFunPlugin.Joke\\ndescription: Generate a funny joke\\nargs:\\n- input: the input to generate a joke about\\n\\n[GOAL]\\n\"Tell a joke about cars. Translate it to Spanish\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"cars\",\\n        \"subtasks\": [\\n            {\"function\": \"FunPlugin.Joke\"},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"Spanish\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nWriterPlugin.Brainstorm\\ndescription: Brainstorm ideas\\nargs:\\n- input: the input to brainstorm about\\n\\nEdgarAllenPoePlugin.Poe\\ndescription: Write in the style of author Edgar Allen Poe\\nargs:\\n- input: the input to write about\\n\\nWriterPlugin.EmailTo\\ndescription: Write an email to a recipient\\nargs:\\n- input: the input to write about\\n- recipient: the recipient\\'s email address.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\n[GOAL]\\n\"Tomorrow is Valentine\\'s day. I need to come up with a few date ideas.\\nShe likes Edgar Allen Poe so write using his style.\\nE-mail these ideas to my significant other. Translate it to French.\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"Valentine\\'s Day Date Ideas\",\\n        \"subtasks\": [\\n            {\"function\": \"WriterPlugin.Brainstorm\"},\\n            {\"function\": \"EdgarAllenPoePlugin.Poe\"},\\n            {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"French\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nTextPlugin.lowercase\\ndescription: Convert a string to lowercase.\\nargs:\\n\\nTextPlugin.trim\\ndescription: Trim whitespace from the start and end of a string.\\nargs:\\n\\nTextPlugin.trim_end\\ndescription: Trim whitespace from the end of a string.\\nargs:\\n\\nTextPlugin.trim_start\\ndescription: Trim whitespace from the start of a string.\\nargs:\\n\\nTextPlugin.uppercase\\ndescription: Convert a string to uppercase.\\nargs:\\n\\nSummarizePlugin.MakeAbstractReadable\\ndescription: Given a scientific white paper abstract, rewrite it to make it more readable\\nargs:\\n- input: \\n\\nSummarizePlugin.Notegen\\ndescription: Automatically generate compact notes for any text or text document.\\nargs:\\n- input: \\n\\nSummarizePlugin.Summarize\\ndescription: Summarize given text or any text document\\nargs:\\n- input: Text to summarize\\n\\nSummarizePlugin.Topics\\ndescription: Analyze given text or document and extract key topics worth remembering\\nargs:\\n- input: \\n\\nWriterPlugin.Acronym\\ndescription: Generate an acronym for the given concept or phrase\\nargs:\\n- input: \\n\\nWriterPlugin.AcronymGenerator\\ndescription: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.AcronymReverse\\ndescription: Given a single word or acronym, generate the expanded form matching the acronym letters.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.Brainstorm\\ndescription: Given a goal or topic description generate a list of ideas\\nargs:\\n- input: A topic description or goal.\\n- INPUT: \\n\\nWriterPlugin.EmailGen\\ndescription: Write an email from the given bullet points\\nargs:\\n- input: \\n\\nWriterPlugin.EmailTo\\ndescription: Turn bullet points into an email to someone, using a polite tone\\nargs:\\n- to: \\n- input: \\n- sender: \\n\\nWriterPlugin.EnglishImprover\\ndescription: Translate text to English and improve it\\nargs:\\n- INPUT: \\n\\nWriterPlugin.NovelChapter\\ndescription: Write a chapter of a novel.\\nargs:\\n- input: A synopsis of what the chapter should be about.\\n- theme: The theme or topic of this novel.\\n- previousChapter: The synopsis of the previous chapter.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelChapterWithNotes\\ndescription: Write a chapter of a novel using notes about the chapter to write.\\nargs:\\n- input: What the novel should be about.\\n- theme: The theme of this novel.\\n- notes: Notes useful to write this chapter.\\n- previousChapter: The previous chapter synopsis.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline\\ndescription: Generate a list of chapter synopsis for a novel or novella\\nargs:\\n- input: What the novel should be about.\\n- chapterCount: The number of chapters to generate.\\n- endMarker: The marker to use to end each chapter.\\n\\nWriterPlugin.Rewrite\\ndescription: Automatically generate compact notes for any text or text document\\nargs:\\n- style: \\n- input: \\n\\nWriterPlugin.ShortPoem\\ndescription: Turn a scenario into a short and entertaining poem.\\nargs:\\n- input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen\\ndescription: Generate a list of synopsis for a novel or novella with sub-chapters\\nargs:\\n- input: \\n\\nWriterPlugin.TellMeMore\\ndescription: Summarize given text or any text document\\nargs:\\n- conversationtype: \\n- input: \\n- focusarea: \\n- previousresults: \\n\\nWriterPlugin.Translate\\ndescription: Translate the input into a language of your choice\\nargs:\\n- input: Text to translate\\n- language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary\\ndescription: Summarize given text in two sentences or less\\nargs:\\n- input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\ndescription: Generic function, unknown purpose\\nargs:\\n- available_functions: \\n- goal: \\n\\n\\n\\n[GOAL]\\n\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n\\n\\n[OUTPUT]\\n'}], extra_body=None), chat_prompt_template=None)}), 'SequentialPlanner_Excluded': KernelPlugin(name='SequentialPlanner_Excluded', description=None, functions={'SequentialPlanner_Excluded': KernelFunction(plugin_name='SequentialPlanner_Excluded', description='Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow', name='SequentialPlanner_Excluded', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969B4C0>, parameters=[ParameterView(name='input', description='The question to answer', default_value='', type_=None, required=False), ParameterView(name='available_functions', description=\"The list of the agent's available_functions\", default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Create an XML plan step by step, to satisfy the goal given, with the available functions.\\n\\n[AVAILABLE FUNCTIONS]\\n\\nSummarizePlugin.MakeAbstractReadable:\\n  description: Given a scientific white paper abstract, rewrite it to make it more readable\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Notegen:\\n  description: Automatically generate compact notes for any text or text document.\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Summarize:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - input: Text to summarize\\n\\nSummarizePlugin.Topics:\\n  description: Analyze given text or document and extract key topics worth remembering\\n  inputs:\\n    - input: \\n\\nWriterPlugin.Acronym:\\n  description: Generate an acronym for the given concept or phrase\\n  inputs:\\n    - input: \\n\\nWriterPlugin.AcronymGenerator:\\n  description: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.AcronymReverse:\\n  description: Given a single word or acronym, generate the expanded form matching the acronym letters.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.Brainstorm:\\n  description: Given a goal or topic description generate a list of ideas\\n  inputs:\\n    - input: A topic description or goal.\\n  - INPUT: \\n\\nWriterPlugin.EmailGen:\\n  description: Write an email from the given bullet points\\n  inputs:\\n    - input: \\n\\nWriterPlugin.EmailTo:\\n  description: Turn bullet points into an email to someone, using a polite tone\\n  inputs:\\n    - to: \\n  - input: \\n  - sender: \\n\\nWriterPlugin.EnglishImprover:\\n  description: Translate text to English and improve it\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.NovelChapter:\\n  description: Write a chapter of a novel.\\n  inputs:\\n    - input: A synopsis of what the chapter should be about.\\n  - theme: The theme or topic of this novel.\\n  - previousChapter: The synopsis of the previous chapter.\\n  - chapterIndex: The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.NovelChapterWithNotes:\\n  description: Write a chapter of a novel using notes about the chapter to write.\\n  inputs:\\n    - input: What the novel should be about.\\n  - theme: The theme of this novel.\\n  - notes: Notes useful to write this chapter.\\n  - previousChapter: The previous chapter synopsis.\\n  - chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline:\\n  description: Generate a list of chapter synopsis for a novel or novella\\n  inputs:\\n    - input: What the novel should be about.\\n  - chapterCount: The number of chapters to generate.\\n  - endMarker: The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.Rewrite:\\n  description: Automatically generate compact notes for any text or text document\\n  inputs:\\n    - style: \\n  - input: \\n\\nWriterPlugin.ShortPoem:\\n  description: Turn a scenario into a short and entertaining poem.\\n  inputs:\\n    - input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen:\\n  description: Generate a list of synopsis for a novel or novella with sub-chapters\\n  inputs:\\n    - input: \\n\\nWriterPlugin.TellMeMore:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - conversationtype: \\n  - input: \\n  - focusarea: \\n  - previousresults: \\n\\nWriterPlugin.Translate:\\n  description: Translate the input into a language of your choice\\n  inputs:\\n    - input: Text to translate\\n  - language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary:\\n  description: Summarize given text in two sentences or less\\n  inputs:\\n    - input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg:\\n  description: Generic function, unknown purpose\\n  inputs:\\n    - available_functions: \\n  - goal: \\n\\nTextPlugin.lowercase:\\n  description: Convert a string to lowercase.\\n  inputs:\\n  \\n\\nTextPlugin.trim:\\n  description: Trim whitespace from the start and end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_end:\\n  description: Trim whitespace from the end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_start:\\n  description: Trim whitespace from the start of a string.\\n  inputs:\\n  \\n\\nTextPlugin.uppercase:\\n  description: Convert a string to uppercase.\\n  inputs:\\n  \\n\\n[END AVAILABLE FUNCTIONS]\\n\\nTo create a plan, follow these steps:\\n0. The plan should be as short as possible.\\n1. From a <goal> create a <plan> as a series of <functions>.\\n2. A plan has \\'INPUT\\' available in context variables by default.\\n3. Before using any function in a plan, check that it is present in the [AVAILABLE FUNCTIONS] list. If it is not, do not use it.\\n4. Only use functions that are required for the given goal.\\n5. Append an \"END\" XML comment at the end of the plan after the final closing </plan> tag.\\n6. Always output valid XML that can be parsed by an XML parser.\\n7. If a plan cannot be created with the [AVAILABLE FUNCTIONS], return <plan />.\\n\\nAll plans take the form of:\\n<plan>\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    (... etc ...)\\n</plan>\\n<!-- END -->\\n\\nTo call a function, follow these steps:\\n1. A function has one or more named parameters and a single \\'output\\' which are all strings. Parameter values should be xml escaped.\\n2. To save an \\'output\\' from a <function>, to pass into a future <function>, use <function.{FullyQualifiedFunctionName} ... setContextVariable=\"<UNIQUE_VARIABLE_KEY>\"/>\\n3. To save an \\'output\\' from a <function>, to return as part of a plan result, use <function.{FullyQualifiedFunctionName} ... appendToResult=\"RESULT__<UNIQUE_RESULT_KEY>\"/>\\n4. Use a \\'$\\' to reference a context variable in a parameter, e.g. when `INPUT=\\'world\\'` the parameter \\'Hello $INPUT\\' will evaluate to `Hello world`.\\n5. Functions do not have access to the context variables of other functions. Do not attempt to use context variables as arrays or objects. Instead, use available functions to extract specific elements or properties from context variables.\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS NOT XML ESCAPED:\\n<function.Name4 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a <!-- \\'comment\\' in it-->\"/>\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS ATTEMPTING TO USE A CONTEXT VARIABLE AS AN ARRAY/OBJECT:\\n<function.CallFunction input=\"$OTHER_OUTPUT[1]\"/>\\n\\nHere is a valid example of how to call a function \"_Function_.Name\" with a single input and save its output:\\n<function._Function_.Name input=\"this is my input\" setContextVariable=\"SOME_KEY\"/>\\n\\nHere is a valid example of how to call a function \"FunctionName2\" with a single input and return its output as part of the plan result:\\n<function.FunctionName2 input=\"Hello $INPUT\" appendToResult=\"RESULT__FINAL_ANSWER\"/>\\n\\nHere is a valid example of how to call a function \"Name3\" with multiple inputs:\\n<function.Name3 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a &lt;!-- &apos;comment&apos; in it--&gt;\"/>\\n\\nBegin!\\n\\n<goal>\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n</goal>\\n'}], extra_body=None), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_vvSKvltRilNtxOYw': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_vvSKvltRilNtxOYw', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969BF70>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'A planner takes a list of functions, a goal, and chooses which function to use.\\nFor each function the list includes details about the input parameters.\\n[START OF EXAMPLES]\\n\\n[EXAMPLE]\\n- List of functions:\\n// Read a file.\\nFileIOPlugin.ReadAsync\\nParameter \"\"path\"\": Source file.\\n// Write a file.\\nFileIOPlugin.WriteAsync\\nParameter \"\"path\"\": Destination file. (default value: sample.txt)\\nParameter \"\"content\"\": File content.\\n// Get the current time.\\nTimePlugin.Time\\nNo parameters.\\n// Makes a POST request to a uri.\\nHttpPlugin.PostAsync\\nParameter \"\"body\"\": The body of the request.\\n- End list of functions.\\nGoal: create a file called \"\"something.txt\"\".\\n{\"\"plan\"\":{\\n\"\"rationale\"\": \"\"the list contains a function that allows to create files\"\",\\n\"\"function\"\": \"\"FileIOPlugin.WriteAsync\"\",\\n\"\"parameters\"\": {\\n\"\"path\"\": \"\"something.txt\"\",\\n\"\"content\"\": null\\n}}}\\n#END-OF-PLAN\\n\\n\\n[EXAMPLE]\\n- List of functions:\\n// Get the current time.\\nTimePlugin.Time\\nNo parameters.\\n// Write a file.\\nFileIOPlugin.WriteAsync\\nParameter \"\"path\"\": Destination file. (default value: sample.txt)\\nParameter \"\"content\"\": File content.\\n// Makes a POST request to a uri.\\nHttpPlugin.PostAsync\\nParameter \"\"body\"\": The body of the request.\\n// Read a file.\\nFileIOPlugin.ReadAsync\\nParameter \"\"path\"\": Source file.\\n- End list of functions.\\nGoal: tell me a joke.\\n{\"\"plan\"\":{\\n\"\"rationale\"\": \"\"the list does not contain functions to tell jokes or something funny\"\",\\n\"\"function\"\": \"\"\"\",\\n\"\"parameters\"\": {\\n}}}\\n#END-OF-PLAN\\n\\n[END OF EXAMPLES]\\n[REAL SCENARIO STARTS HERE]\\n- List of functions:\\n// Given a scientific white paper abstract, rewrite it to make it more readable.\\nSummarizePlugin.MakeAbstractReadable\\nParameter \"\"input\"\": input.\\n// Automatically generate compact notes for any text or text document.\\nSummarizePlugin.Notegen\\nParameter \"\"input\"\": input.\\n// Summarize given text or any text document.\\nSummarizePlugin.Summarize\\nParameter \"\"input\"\": Text to summarize.\\n// Analyze given text or document and extract key topics worth remembering.\\nSummarizePlugin.Topics\\nParameter \"\"input\"\": input.\\n// Generate an acronym for the given concept or phrase.\\nWriterPlugin.Acronym\\nParameter \"\"input\"\": input.\\n// Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nWriterPlugin.AcronymGenerator\\nParameter \"\"INPUT\"\": INPUT.\\n// Given a single word or acronym, generate the expanded form matching the acronym letters.\\nWriterPlugin.AcronymReverse\\nParameter \"\"INPUT\"\": INPUT.\\n// Given a goal or topic description generate a list of ideas.\\nWriterPlugin.Brainstorm\\nParameter \"\"input\"\": A topic description or goal.\\nParameter \"\"INPUT\"\": INPUT.\\n// Write an email from the given bullet points.\\nWriterPlugin.EmailGen\\nParameter \"\"input\"\": input.\\n// Turn bullet points into an email to someone, using a polite tone.\\nWriterPlugin.EmailTo\\nParameter \"\"to\"\": to.\\nParameter \"\"input\"\": input.\\nParameter \"\"sender\"\": sender.\\n// Translate text to English and improve it.\\nWriterPlugin.EnglishImprover\\nParameter \"\"INPUT\"\": INPUT.\\n// Write a chapter of a novel.\\nWriterPlugin.NovelChapter\\nParameter \"\"input\"\": A synopsis of what the chapter should be about.\\nParameter \"\"theme\"\": The theme or topic of this novel.\\nParameter \"\"previousChapter\"\": The synopsis of the previous chapter.\\nParameter \"\"chapterIndex\"\": The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n// Write a chapter of a novel using notes about the chapter to write.\\nWriterPlugin.NovelChapterWithNotes\\nParameter \"\"input\"\": What the novel should be about.\\nParameter \"\"theme\"\": The theme of this novel.\\nParameter \"\"notes\"\": Notes useful to write this chapter.\\nParameter \"\"previousChapter\"\": The previous chapter synopsis.\\nParameter \"\"chapterIndex\"\": The number of the chapter to write.\\n// Generate a list of chapter synopsis for a novel or novella.\\nWriterPlugin.NovelOutline\\nParameter \"\"input\"\": What the novel should be about.\\nParameter \"\"chapterCount\"\": The number of chapters to generate.\\nParameter \"\"endMarker\"\": The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n// Automatically generate compact notes for any text or text document.\\nWriterPlugin.Rewrite\\nParameter \"\"style\"\": style.\\nParameter \"\"input\"\": input.\\n// Turn a scenario into a short and entertaining poem.\\nWriterPlugin.ShortPoem\\nParameter \"\"input\"\": The scenario to turn into a poem.\\n// Generate a list of synopsis for a novel or novella with sub-chapters.\\nWriterPlugin.StoryGen\\nParameter \"\"input\"\": input.\\n// Summarize given text or any text document.\\nWriterPlugin.TellMeMore\\nParameter \"\"conversationtype\"\": conversationtype.\\nParameter \"\"input\"\": input.\\nParameter \"\"focusarea\"\": focusarea.\\nParameter \"\"previousresults\"\": previousresults.\\n// Translate the input into a language of your choice.\\nWriterPlugin.Translate\\nParameter \"\"input\"\": Text to translate.\\nParameter \"\"language\"\": Language to translate to.\\n// Summarize given text in two sentences or less.\\nWriterPlugin.TwoSentenceSummary\\nParameter \"\"input\"\": input.\\n// Generic function, unknown purpose.\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\nParameter \"\"available_functions\"\": available_functions.\\nParameter \"\"goal\"\": goal.\\n// Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow.\\nSequentialPlanner_Excluded.SequentialPlanner_Excluded\\nParameter \"\"input\"\": The question to answer.\\nParameter \"\"available_functions\"\": The list of the agent\\'s available_functions.\\n// Convert a string to lowercase.\\nTextPlugin.lowercase\\nNo parameters.\\n// Trim whitespace from the start and end of a string.\\nTextPlugin.trim\\nNo parameters.\\n// Trim whitespace from the end of a string.\\nTextPlugin.trim_end\\nNo parameters.\\n// Trim whitespace from the start of a string.\\nTextPlugin.trim_start\\nNo parameters.\\n// Convert a string to uppercase.\\nTextPlugin.uppercase\\nNo parameters.\\n// Adds value to a value.\\nmath.Add\\nParameter \"\"input\"\": The value to add.\\nParameter \"\"Amount\"\": Amount to add.\\n// Subtracts value to a value.\\nmath.Subtract\\nParameter \"\"input\"\": The value to subtract.\\nParameter \"\"Amount\"\": Amount to subtract.\\n// Read a file.\\nfileIO.readAsync\\nParameter \"\"input\"\": Path of the source file.\\n// Write a file.\\nfileIO.writeAsync\\nParameter \"\"content\"\": File content.\\nParameter \"\"path\"\": Destination path.\\n// Get the current date.\\ntime.date\\nNo parameters.\\n// Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era \\'Martedi\\' scorso -> dateMatchingLastDayName \\'Tuesday\\' => Tuesday,\\n        16 May, 2023.\\ntime.date_matching_last_day_name\\nNo parameters.\\n// Get the current day.\\ntime.day\\nNo parameters.\\n// Get the current day of the week.\\ntime.dayOfWeek\\nNo parameters.\\n// Get the date of offset from today by a provided number of days.\\ntime.days_ago\\nNo parameters.\\n// Get the current hour.\\ntime.hour\\nNo parameters.\\n// Get the current hour number.\\ntime.hourNumber\\nNo parameters.\\n// Get the current date in iso format.\\ntime.iso_date\\nNo parameters.\\n// Get the current minute.\\ntime.minute\\nNo parameters.\\n// Get the current month.\\ntime.month\\nNo parameters.\\n// Get the current month number.\\ntime.month_number\\nNo parameters.\\n// Get the current date and time in the local time zone.\\ntime.now\\nNo parameters.\\n// Get the seconds on the current minute.\\ntime.second\\nNo parameters.\\n// Get the current time in the local time zone.\\ntime.time\\nNo parameters.\\n// Get the current time zone name.\\ntime.timeZoneName\\nNo parameters.\\n// Get the current time zone offset.\\ntime.timeZoneOffset\\nNo parameters.\\n// Get the current date.\\ntime.today\\nNo parameters.\\n// Get the current date and time in UTC.\\ntime.utcNow\\nNo parameters.\\n// Get the current year.\\ntime.year\\nNo parameters.\\n// Convert a string to lowercase.\\ntext.lowercase\\nNo parameters.\\n// Trim whitespace from the start and end of a string.\\ntext.trim\\nNo parameters.\\n// Trim whitespace from the end of a string.\\ntext.trim_end\\nNo parameters.\\n// Trim whitespace from the start of a string.\\ntext.trim_start\\nNo parameters.\\n// Convert a string to uppercase.\\ntext.uppercase\\nNo parameters.\\n- End list of functions.\\nGoal: what is the sum of 22 and 22?\\n'}], extra_body=None), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'fileIO': KernelPlugin(name='fileIO', description=None, functions={'readAsync': KernelFunction(plugin_name='fileIO', description='Read a file', name='readAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.read of FileIOPlugin()>, parameters=[ParameterView(name='input', description='Path of the source file', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringOutTaskString: 10>, function=<bound method FileIOPlugin.read of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'writeAsync': KernelFunction(plugin_name='fileIO', description='Write a file', name='writeAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.write of FileIOPlugin()>, parameters=[ParameterView(name='content', description='File content', default_value='', type_='string', required=False), ParameterView(name='path', description='Destination path', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InContextOutTask: 16>, function=<bound method FileIOPlugin.write of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'time': KernelPlugin(name='time', description=None, functions={'date': KernelFunction(plugin_name='time', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='time', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='time', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='time', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='time', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='time', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='time', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='time', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='time', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='time', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='time', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='time', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='time', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='time', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='time', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='time', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='time', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='time', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='time', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'text': KernelPlugin(name='text', description=None, functions={'lowercase': KernelFunction(plugin_name='text', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='text', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='text', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='text', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='text', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'timee': KernelPlugin(name='timee', description=None, functions={'date': KernelFunction(plugin_name='timee', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='timee', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='timee', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='timee', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='timee', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='timee', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='timee', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='timee', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='timee', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='timee', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='timee', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='timee', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='timee', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='timee', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='timee', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='timee', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='timee', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='timee', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='timee', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'mathh': KernelPlugin(name='mathh', description=None, functions={...})}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='mathh', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={'SummarizePlugin': KernelPlugin(name='SummarizePlugin', description=None, functions={'MakeAbstractReadable': KernelFunction(plugin_name='SummarizePlugin', description='Given a scientific white paper abstract, rewrite it to make it more readable', name='MakeAbstractReadable', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7C8E19DC0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 2.0}, ai_model_id=None, frequency_penalty=2.0, logit_bias={}, max_tokens=4000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Notegen': KernelFunction(plugin_name='SummarizePlugin', description='Automatically generate compact notes for any text or text document.', name='Notegen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B0D0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E94F4AF0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Summarize': KernelFunction(plugin_name='SummarizePlugin', description='Summarize given text or any text document', name='Summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E8A4FEE0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E8A7BDC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Topics': KernelFunction(plugin_name='SummarizePlugin', description='Analyze given text or document and extract key topics worth remembering', name='Topics', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B280>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B1F0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 128, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=128, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'Acronym': KernelFunction(plugin_name='WriterPlugin', description='Generate an acronym for the given concept or phrase', name='Acronym', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B940>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9445A60>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymGenerator': KernelFunction(plugin_name='WriterPlugin', description='Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.', name='AcronymGenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954B700>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.7, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'AcronymReverse': KernelFunction(plugin_name='WriterPlugin', description='Given a single word or acronym, generate the expanded form matching the acronym letters.', name='AcronymReverse', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BA60>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B9D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.8, 'frequency_penalty': 0.0, 'stop_sequences': ['#END#']}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.8, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Brainstorm': KernelFunction(plugin_name='WriterPlugin', description='Given a goal or topic description generate a list of ideas', name='Brainstorm', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BDC0>, parameters=[ParameterView(name='input', description='A topic description or goal.', default_value='', type_=None, required=False), ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.5, 'top_p': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['##END##']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Must: brainstorm ideas and create a list.\\nMust: use a numbered list.\\nMust: only one list.\\nMust: end list with ##END##\\nShould: no more than 10 items.\\nShould: at least 3 items.\\nTopic: Valentine's Day Date Ideas\\nStart.\\n\"}], extra_body=None), chat_prompt_template=None), 'EmailGen': KernelFunction(plugin_name='WriterPlugin', description='Write an email from the given bullet points', name='EmailGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BD30>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BC10>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'EmailTo': KernelFunction(plugin_name='WriterPlugin', description='Turn bullet points into an email to someone, using a polite tone', name='EmailTo', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BEE0>, parameters=[ParameterView(name='to', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='sender', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BE50>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Rewrite my bullet points into an email featuring complete sentences. Use a polite and inclusive tone.  \\n\\n[Input]\\nToby,\\n\\n- Macbeth, King Scotland\\n- Married, Wife Lady Macbeth, No Kids\\n- Dog Toby McDuff. Hunter, dead. \\n- Shakespeare play\\n\\nThanks,\\nDexter\\n\\n+++++\\nHi Toby,\\n\\nThe story of Macbeth\\nMy name is Macbeth. I used to be King of Scotland, but I died. My wife's name is Lady Macbeth and we were married for 15 years. We had no children. Our beloved dog Toby McDuff was a famous hunter of rats in the forest.\\nMy story was immortalized by Shakespeare in a play.\\n\\nThanks,\\nDexter\\n\\n+++++\\n[Input]\\n\\n1. Picnic in the park\\n2. Cooking a romantic dinner together\\n3. Going to a wine tasting\\n4. Ice skating\\n5. Taking a hot air balloon ride\\n6. Watching a romantic movie at home\\n7. Going on a hike and having a picnic at the top\\n8. Taking a dance class together\\n9. Visiting a local museum or art gallery\\n10. Having a spa day together\\n##END##\\n\\nThanks,\\n\\n+++++\\n\"}], extra_body=None), chat_prompt_template=None), 'EnglishImprover': KernelFunction(plugin_name='WriterPlugin', description='Translate text to English and improve it', name='EnglishImprover', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E954BCA0>, parameters=[ParameterView(name='INPUT', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954BF70>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 3000, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=3000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapter': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel.', name='NovelChapter', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556040>, parameters=[ParameterView(name='input', description='A synopsis of what the chapter should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme or topic of this novel.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The synopsis of the previous chapter.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556160>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.3, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.3, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelChapterWithNotes': KernelFunction(plugin_name='WriterPlugin', description='Write a chapter of a novel using notes about the chapter to write.', name='NovelChapterWithNotes', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95561F0>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='theme', description='The theme of this novel.', default_value='', type_=None, required=False), ParameterView(name='notes', description='Notes useful to write this chapter.', default_value='', type_=None, required=False), ParameterView(name='previousChapter', description='The previous chapter synopsis.', default_value='', type_=None, required=False), ParameterView(name='chapterIndex', description='The number of the chapter to write.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95560D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 1024, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'NovelOutline': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of chapter synopsis for a novel or novella', name='NovelOutline', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512A60>, parameters=[ParameterView(name='input', description='What the novel should be about.', default_value='', type_=None, required=False), ParameterView(name='chapterCount', description='The number of chapters to generate.', default_value='', type_=None, required=False), ParameterView(name='endMarker', description='The marker to use to end each chapter.', default_value='<!--===ENDPART===-->', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512EE0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2048, 'temperature': 0.1, 'top_p': 0.5, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=2048, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.1, top_p=0.5, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Rewrite': KernelFunction(plugin_name='WriterPlugin', description='Automatically generate compact notes for any text or text document', name='Rewrite', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512B80>, parameters=[ParameterView(name='style', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9512DC0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 256, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'ShortPoem': KernelFunction(plugin_name='WriterPlugin', description='Turn a scenario into a short and entertaining poem.', name='ShortPoem', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9512AF0>, parameters=[ParameterView(name='input', description='The scenario to turn into a poem.', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95129D0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 60, 'temperature': 0.5, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=60, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.5, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'StoryGen': KernelFunction(plugin_name='WriterPlugin', description='Generate a list of synopsis for a novel or novella with sub-chapters', name='StoryGen', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556430>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556280>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 250, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=250, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'TellMeMore': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='TellMeMore', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95564C0>, parameters=[ParameterView(name='conversationtype', description='', default_value='', type_='string', required=False), ParameterView(name='input', description='', default_value='', type_='string', required=False), ParameterView(name='focusarea', description='', default_value='', type_='string', required=False), ParameterView(name='previousresults', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556310>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 500, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=500, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None), 'Translate': KernelFunction(plugin_name='WriterPlugin', description='Translate the input into a language of your choice', name='Translate', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E9556670>, parameters=[ParameterView(name='input', description='Text to translate', default_value='', type_=None, required=False), ParameterView(name='language', description='Language to translate to', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E95565E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 2000, 'temperature': 0.7, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop_sequences': ['[done]']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=2000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.7, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': \"Translate the input below into English\\n\\nMAKE SURE YOU ONLY USE English.\\n\\nHi there,\\n\\nI hope this email finds you well. I wanted to share some ideas for fun and romantic activities that we could do together:\\n\\n1. We could have a lovely picnic in the park, surrounded by nature and fresh air.\\n2. Cooking a romantic dinner together could be a fun and intimate way to spend an evening.\\n3. Going to a wine tasting would be a great opportunity to try new wines and learn more about them.\\n4. Ice skating is always a fun and playful activity that we could enjoy together.\\n5. Taking a hot air balloon ride would be a unique and exciting experience that we could share.\\n6. We could also have a cozy night in and watch a romantic movie together.\\n7. Going on a hike and having a picnic at the top would be a great way to enjoy the outdoors and each other's company.\\n8. Taking a dance class together could be a fun and romantic way to learn something new.\\n9. Visiting a local museum or art gallery would be a great way to appreciate art and culture together.\\n10. Finally, we could have a relaxing spa day together and enjoy some pampering and relaxation.\\n\\nLet me know what you think and if you have any other ideas!\\n\\nThanks,\\n[Your Name]\\n\\nTranslation:\\n\"}], extra_body=None), chat_prompt_template=None), 'TwoSentenceSummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in two sentences or less', name='TwoSentenceSummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E95563A0>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E9556790>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None, extra_body=None), chat_prompt_template=None)}), 'TextPlugin': KernelPlugin(name='TextPlugin', description=None, functions={'lowercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='TextPlugin', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='TextPlugin', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_tSNsOTIMHWAhndYw': KernelPlugin(name='p_tSNsOTIMHWAhndYw', description=None, functions={'f_QOkQhOcvcIJkhbYg': KernelFunction(plugin_name='p_tSNsOTIMHWAhndYw', description='Generic function, unknown purpose', name='f_QOkQhOcvcIJkhbYg', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E964DF70>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E954B550>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\\nYou are a planner for the Semantic Kernel.\\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\\nCreate a list of subtasks based off the [GOAL] provided.\\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\\nBase your decisions on which functions to use from the description and the name of the function.\\nSometimes, a function may take arguments. Provide them if necessary.\\nThe plan should be as short as possible.\\nFor example:\\n\\n[AVAILABLE FUNCTIONS]\\nEmailConnector.LookupContactEmail\\ndescription: looks up the a contact and retrieves their email address\\nargs:\\n- name: the name to look up\\n\\nWriterPlugin.EmailTo\\ndescription: email the input text to a recipient\\nargs:\\n- input: the text to email\\n- recipient: the recipient\\'s email address. Multiple addresses may be included if separated by \\';\\'.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\nWriterPlugin.Summarize\\ndescription: summarize input text\\nargs:\\n- input: the text to summarize\\n\\nFunPlugin.Joke\\ndescription: Generate a funny joke\\nargs:\\n- input: the input to generate a joke about\\n\\n[GOAL]\\n\"Tell a joke about cars. Translate it to Spanish\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"cars\",\\n        \"subtasks\": [\\n            {\"function\": \"FunPlugin.Joke\"},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"Spanish\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nWriterPlugin.Brainstorm\\ndescription: Brainstorm ideas\\nargs:\\n- input: the input to brainstorm about\\n\\nEdgarAllenPoePlugin.Poe\\ndescription: Write in the style of author Edgar Allen Poe\\nargs:\\n- input: the input to write about\\n\\nWriterPlugin.EmailTo\\ndescription: Write an email to a recipient\\nargs:\\n- input: the input to write about\\n- recipient: the recipient\\'s email address.\\n\\nWriterPlugin.Translate\\ndescription: translate the input to another language\\nargs:\\n- input: the text to translate\\n- language: the language to translate to\\n\\n[GOAL]\\n\"Tomorrow is Valentine\\'s day. I need to come up with a few date ideas.\\nShe likes Edgar Allen Poe so write using his style.\\nE-mail these ideas to my significant other. Translate it to French.\"\\n\\n[OUTPUT]\\n    {\\n        \"input\": \"Valentine\\'s Day Date Ideas\",\\n        \"subtasks\": [\\n            {\"function\": \"WriterPlugin.Brainstorm\"},\\n            {\"function\": \"EdgarAllenPoePlugin.Poe\"},\\n            {\"function\": \"WriterPlugin.EmailTo\", \"args\": {\"recipient\": \"significant_other\"}},\\n            {\"function\": \"WriterPlugin.Translate\", \"args\": {\"language\": \"French\"}}\\n        ]\\n    }\\n\\n[AVAILABLE FUNCTIONS]\\nTextPlugin.lowercase\\ndescription: Convert a string to lowercase.\\nargs:\\n\\nTextPlugin.trim\\ndescription: Trim whitespace from the start and end of a string.\\nargs:\\n\\nTextPlugin.trim_end\\ndescription: Trim whitespace from the end of a string.\\nargs:\\n\\nTextPlugin.trim_start\\ndescription: Trim whitespace from the start of a string.\\nargs:\\n\\nTextPlugin.uppercase\\ndescription: Convert a string to uppercase.\\nargs:\\n\\nSummarizePlugin.MakeAbstractReadable\\ndescription: Given a scientific white paper abstract, rewrite it to make it more readable\\nargs:\\n- input: \\n\\nSummarizePlugin.Notegen\\ndescription: Automatically generate compact notes for any text or text document.\\nargs:\\n- input: \\n\\nSummarizePlugin.Summarize\\ndescription: Summarize given text or any text document\\nargs:\\n- input: Text to summarize\\n\\nSummarizePlugin.Topics\\ndescription: Analyze given text or document and extract key topics worth remembering\\nargs:\\n- input: \\n\\nWriterPlugin.Acronym\\ndescription: Generate an acronym for the given concept or phrase\\nargs:\\n- input: \\n\\nWriterPlugin.AcronymGenerator\\ndescription: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.AcronymReverse\\ndescription: Given a single word or acronym, generate the expanded form matching the acronym letters.\\nargs:\\n- INPUT: \\n\\nWriterPlugin.Brainstorm\\ndescription: Given a goal or topic description generate a list of ideas\\nargs:\\n- input: A topic description or goal.\\n- INPUT: \\n\\nWriterPlugin.EmailGen\\ndescription: Write an email from the given bullet points\\nargs:\\n- input: \\n\\nWriterPlugin.EmailTo\\ndescription: Turn bullet points into an email to someone, using a polite tone\\nargs:\\n- to: \\n- input: \\n- sender: \\n\\nWriterPlugin.EnglishImprover\\ndescription: Translate text to English and improve it\\nargs:\\n- INPUT: \\n\\nWriterPlugin.NovelChapter\\ndescription: Write a chapter of a novel.\\nargs:\\n- input: A synopsis of what the chapter should be about.\\n- theme: The theme or topic of this novel.\\n- previousChapter: The synopsis of the previous chapter.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelChapterWithNotes\\ndescription: Write a chapter of a novel using notes about the chapter to write.\\nargs:\\n- input: What the novel should be about.\\n- theme: The theme of this novel.\\n- notes: Notes useful to write this chapter.\\n- previousChapter: The previous chapter synopsis.\\n- chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline\\ndescription: Generate a list of chapter synopsis for a novel or novella\\nargs:\\n- input: What the novel should be about.\\n- chapterCount: The number of chapters to generate.\\n- endMarker: The marker to use to end each chapter.\\n\\nWriterPlugin.Rewrite\\ndescription: Automatically generate compact notes for any text or text document\\nargs:\\n- style: \\n- input: \\n\\nWriterPlugin.ShortPoem\\ndescription: Turn a scenario into a short and entertaining poem.\\nargs:\\n- input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen\\ndescription: Generate a list of synopsis for a novel or novella with sub-chapters\\nargs:\\n- input: \\n\\nWriterPlugin.TellMeMore\\ndescription: Summarize given text or any text document\\nargs:\\n- conversationtype: \\n- input: \\n- focusarea: \\n- previousresults: \\n\\nWriterPlugin.Translate\\ndescription: Translate the input into a language of your choice\\nargs:\\n- input: Text to translate\\n- language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary\\ndescription: Summarize given text in two sentences or less\\nargs:\\n- input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\ndescription: Generic function, unknown purpose\\nargs:\\n- available_functions: \\n- goal: \\n\\n\\n\\n[GOAL]\\n\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n\\n\\n[OUTPUT]\\n'}], extra_body=None), chat_prompt_template=None)}), 'SequentialPlanner_Excluded': KernelPlugin(name='SequentialPlanner_Excluded', description=None, functions={'SequentialPlanner_Excluded': KernelFunction(plugin_name='SequentialPlanner_Excluded', description='Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow', name='SequentialPlanner_Excluded', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969B4C0>, parameters=[ParameterView(name='input', description='The question to answer', default_value='', type_=None, required=False), ParameterView(name='available_functions', description=\"The list of the agent's available_functions\", default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969B8B0>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Create an XML plan step by step, to satisfy the goal given, with the available functions.\\n\\n[AVAILABLE FUNCTIONS]\\n\\nSummarizePlugin.MakeAbstractReadable:\\n  description: Given a scientific white paper abstract, rewrite it to make it more readable\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Notegen:\\n  description: Automatically generate compact notes for any text or text document.\\n  inputs:\\n    - input: \\n\\nSummarizePlugin.Summarize:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - input: Text to summarize\\n\\nSummarizePlugin.Topics:\\n  description: Analyze given text or document and extract key topics worth remembering\\n  inputs:\\n    - input: \\n\\nWriterPlugin.Acronym:\\n  description: Generate an acronym for the given concept or phrase\\n  inputs:\\n    - input: \\n\\nWriterPlugin.AcronymGenerator:\\n  description: Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.AcronymReverse:\\n  description: Given a single word or acronym, generate the expanded form matching the acronym letters.\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.Brainstorm:\\n  description: Given a goal or topic description generate a list of ideas\\n  inputs:\\n    - input: A topic description or goal.\\n  - INPUT: \\n\\nWriterPlugin.EmailGen:\\n  description: Write an email from the given bullet points\\n  inputs:\\n    - input: \\n\\nWriterPlugin.EmailTo:\\n  description: Turn bullet points into an email to someone, using a polite tone\\n  inputs:\\n    - to: \\n  - input: \\n  - sender: \\n\\nWriterPlugin.EnglishImprover:\\n  description: Translate text to English and improve it\\n  inputs:\\n    - INPUT: \\n\\nWriterPlugin.NovelChapter:\\n  description: Write a chapter of a novel.\\n  inputs:\\n    - input: A synopsis of what the chapter should be about.\\n  - theme: The theme or topic of this novel.\\n  - previousChapter: The synopsis of the previous chapter.\\n  - chapterIndex: The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.NovelChapterWithNotes:\\n  description: Write a chapter of a novel using notes about the chapter to write.\\n  inputs:\\n    - input: What the novel should be about.\\n  - theme: The theme of this novel.\\n  - notes: Notes useful to write this chapter.\\n  - previousChapter: The previous chapter synopsis.\\n  - chapterIndex: The number of the chapter to write.\\n\\nWriterPlugin.NovelOutline:\\n  description: Generate a list of chapter synopsis for a novel or novella\\n  inputs:\\n    - input: What the novel should be about.\\n  - chapterCount: The number of chapters to generate.\\n  - endMarker: The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n\\nWriterPlugin.Rewrite:\\n  description: Automatically generate compact notes for any text or text document\\n  inputs:\\n    - style: \\n  - input: \\n\\nWriterPlugin.ShortPoem:\\n  description: Turn a scenario into a short and entertaining poem.\\n  inputs:\\n    - input: The scenario to turn into a poem.\\n\\nWriterPlugin.StoryGen:\\n  description: Generate a list of synopsis for a novel or novella with sub-chapters\\n  inputs:\\n    - input: \\n\\nWriterPlugin.TellMeMore:\\n  description: Summarize given text or any text document\\n  inputs:\\n    - conversationtype: \\n  - input: \\n  - focusarea: \\n  - previousresults: \\n\\nWriterPlugin.Translate:\\n  description: Translate the input into a language of your choice\\n  inputs:\\n    - input: Text to translate\\n  - language: Language to translate to\\n\\nWriterPlugin.TwoSentenceSummary:\\n  description: Summarize given text in two sentences or less\\n  inputs:\\n    - input: \\n\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg:\\n  description: Generic function, unknown purpose\\n  inputs:\\n    - available_functions: \\n  - goal: \\n\\nTextPlugin.lowercase:\\n  description: Convert a string to lowercase.\\n  inputs:\\n  \\n\\nTextPlugin.trim:\\n  description: Trim whitespace from the start and end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_end:\\n  description: Trim whitespace from the end of a string.\\n  inputs:\\n  \\n\\nTextPlugin.trim_start:\\n  description: Trim whitespace from the start of a string.\\n  inputs:\\n  \\n\\nTextPlugin.uppercase:\\n  description: Convert a string to uppercase.\\n  inputs:\\n  \\n\\n[END AVAILABLE FUNCTIONS]\\n\\nTo create a plan, follow these steps:\\n0. The plan should be as short as possible.\\n1. From a <goal> create a <plan> as a series of <functions>.\\n2. A plan has \\'INPUT\\' available in context variables by default.\\n3. Before using any function in a plan, check that it is present in the [AVAILABLE FUNCTIONS] list. If it is not, do not use it.\\n4. Only use functions that are required for the given goal.\\n5. Append an \"END\" XML comment at the end of the plan after the final closing </plan> tag.\\n6. Always output valid XML that can be parsed by an XML parser.\\n7. If a plan cannot be created with the [AVAILABLE FUNCTIONS], return <plan />.\\n\\nAll plans take the form of:\\n<plan>\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    <!-- ... reason for taking step ... -->\\n    <function.{FullyQualifiedFunctionName} ... />\\n    (... etc ...)\\n</plan>\\n<!-- END -->\\n\\nTo call a function, follow these steps:\\n1. A function has one or more named parameters and a single \\'output\\' which are all strings. Parameter values should be xml escaped.\\n2. To save an \\'output\\' from a <function>, to pass into a future <function>, use <function.{FullyQualifiedFunctionName} ... setContextVariable=\"<UNIQUE_VARIABLE_KEY>\"/>\\n3. To save an \\'output\\' from a <function>, to return as part of a plan result, use <function.{FullyQualifiedFunctionName} ... appendToResult=\"RESULT__<UNIQUE_RESULT_KEY>\"/>\\n4. Use a \\'$\\' to reference a context variable in a parameter, e.g. when `INPUT=\\'world\\'` the parameter \\'Hello $INPUT\\' will evaluate to `Hello world`.\\n5. Functions do not have access to the context variables of other functions. Do not attempt to use context variables as arrays or objects. Instead, use available functions to extract specific elements or properties from context variables.\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS NOT XML ESCAPED:\\n<function.Name4 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a <!-- \\'comment\\' in it-->\"/>\\n\\nDO NOT DO THIS, THE PARAMETER VALUE IS ATTEMPTING TO USE A CONTEXT VARIABLE AS AN ARRAY/OBJECT:\\n<function.CallFunction input=\"$OTHER_OUTPUT[1]\"/>\\n\\nHere is a valid example of how to call a function \"_Function_.Name\" with a single input and save its output:\\n<function._Function_.Name input=\"this is my input\" setContextVariable=\"SOME_KEY\"/>\\n\\nHere is a valid example of how to call a function \"FunctionName2\" with a single input and return its output as part of the plan result:\\n<function.FunctionName2 input=\"Hello $INPUT\" appendToResult=\"RESULT__FINAL_ANSWER\"/>\\n\\nHere is a valid example of how to call a function \"Name3\" with multiple inputs:\\n<function.Name3 input=\"$SOME_PREVIOUS_OUTPUT\" parameter_name=\"some value with a &lt;!-- &apos;comment&apos; in it--&gt;\"/>\\n\\nBegin!\\n\\n<goal>\\nTommorrow is Valentine\\'s day. I need to come up with a few date ideas. She speaks French so write it in English.\\nConvert the text to uppercase\\n</goal>\\n'}], extra_body=None), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_vvSKvltRilNtxOYw': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_vvSKvltRilNtxOYw', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x000001F7E969BF70>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x000001F7E969BB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=AzureChatCompletion(ai_model_id='atttestgpt35turbo', client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F7C8E68880>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=5608, completion_tokens=766, total_tokens=6374), prompt_execution_settings=AzureChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id='atttestgpt35turbo', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'A planner takes a list of functions, a goal, and chooses which function to use.\\nFor each function the list includes details about the input parameters.\\n[START OF EXAMPLES]\\n\\n[EXAMPLE]\\n- List of functions:\\n// Read a file.\\nFileIOPlugin.ReadAsync\\nParameter \"\"path\"\": Source file.\\n// Write a file.\\nFileIOPlugin.WriteAsync\\nParameter \"\"path\"\": Destination file. (default value: sample.txt)\\nParameter \"\"content\"\": File content.\\n// Get the current time.\\nTimePlugin.Time\\nNo parameters.\\n// Makes a POST request to a uri.\\nHttpPlugin.PostAsync\\nParameter \"\"body\"\": The body of the request.\\n- End list of functions.\\nGoal: create a file called \"\"something.txt\"\".\\n{\"\"plan\"\":{\\n\"\"rationale\"\": \"\"the list contains a function that allows to create files\"\",\\n\"\"function\"\": \"\"FileIOPlugin.WriteAsync\"\",\\n\"\"parameters\"\": {\\n\"\"path\"\": \"\"something.txt\"\",\\n\"\"content\"\": null\\n}}}\\n#END-OF-PLAN\\n\\n\\n[EXAMPLE]\\n- List of functions:\\n// Get the current time.\\nTimePlugin.Time\\nNo parameters.\\n// Write a file.\\nFileIOPlugin.WriteAsync\\nParameter \"\"path\"\": Destination file. (default value: sample.txt)\\nParameter \"\"content\"\": File content.\\n// Makes a POST request to a uri.\\nHttpPlugin.PostAsync\\nParameter \"\"body\"\": The body of the request.\\n// Read a file.\\nFileIOPlugin.ReadAsync\\nParameter \"\"path\"\": Source file.\\n- End list of functions.\\nGoal: tell me a joke.\\n{\"\"plan\"\":{\\n\"\"rationale\"\": \"\"the list does not contain functions to tell jokes or something funny\"\",\\n\"\"function\"\": \"\"\"\",\\n\"\"parameters\"\": {\\n}}}\\n#END-OF-PLAN\\n\\n[END OF EXAMPLES]\\n[REAL SCENARIO STARTS HERE]\\n- List of functions:\\n// Given a scientific white paper abstract, rewrite it to make it more readable.\\nSummarizePlugin.MakeAbstractReadable\\nParameter \"\"input\"\": input.\\n// Automatically generate compact notes for any text or text document.\\nSummarizePlugin.Notegen\\nParameter \"\"input\"\": input.\\n// Summarize given text or any text document.\\nSummarizePlugin.Summarize\\nParameter \"\"input\"\": Text to summarize.\\n// Analyze given text or document and extract key topics worth remembering.\\nSummarizePlugin.Topics\\nParameter \"\"input\"\": input.\\n// Generate an acronym for the given concept or phrase.\\nWriterPlugin.Acronym\\nParameter \"\"input\"\": input.\\n// Given a request to generate an acronym from a string, generate an acronym and provide the acronym explanation.\\nWriterPlugin.AcronymGenerator\\nParameter \"\"INPUT\"\": INPUT.\\n// Given a single word or acronym, generate the expanded form matching the acronym letters.\\nWriterPlugin.AcronymReverse\\nParameter \"\"INPUT\"\": INPUT.\\n// Given a goal or topic description generate a list of ideas.\\nWriterPlugin.Brainstorm\\nParameter \"\"input\"\": A topic description or goal.\\nParameter \"\"INPUT\"\": INPUT.\\n// Write an email from the given bullet points.\\nWriterPlugin.EmailGen\\nParameter \"\"input\"\": input.\\n// Turn bullet points into an email to someone, using a polite tone.\\nWriterPlugin.EmailTo\\nParameter \"\"to\"\": to.\\nParameter \"\"input\"\": input.\\nParameter \"\"sender\"\": sender.\\n// Translate text to English and improve it.\\nWriterPlugin.EnglishImprover\\nParameter \"\"INPUT\"\": INPUT.\\n// Write a chapter of a novel.\\nWriterPlugin.NovelChapter\\nParameter \"\"input\"\": A synopsis of what the chapter should be about.\\nParameter \"\"theme\"\": The theme or topic of this novel.\\nParameter \"\"previousChapter\"\": The synopsis of the previous chapter.\\nParameter \"\"chapterIndex\"\": The number of the chapter to write. (default value: <!--===ENDPART===-->)\\n// Write a chapter of a novel using notes about the chapter to write.\\nWriterPlugin.NovelChapterWithNotes\\nParameter \"\"input\"\": What the novel should be about.\\nParameter \"\"theme\"\": The theme of this novel.\\nParameter \"\"notes\"\": Notes useful to write this chapter.\\nParameter \"\"previousChapter\"\": The previous chapter synopsis.\\nParameter \"\"chapterIndex\"\": The number of the chapter to write.\\n// Generate a list of chapter synopsis for a novel or novella.\\nWriterPlugin.NovelOutline\\nParameter \"\"input\"\": What the novel should be about.\\nParameter \"\"chapterCount\"\": The number of chapters to generate.\\nParameter \"\"endMarker\"\": The marker to use to end each chapter. (default value: <!--===ENDPART===-->)\\n// Automatically generate compact notes for any text or text document.\\nWriterPlugin.Rewrite\\nParameter \"\"style\"\": style.\\nParameter \"\"input\"\": input.\\n// Turn a scenario into a short and entertaining poem.\\nWriterPlugin.ShortPoem\\nParameter \"\"input\"\": The scenario to turn into a poem.\\n// Generate a list of synopsis for a novel or novella with sub-chapters.\\nWriterPlugin.StoryGen\\nParameter \"\"input\"\": input.\\n// Summarize given text or any text document.\\nWriterPlugin.TellMeMore\\nParameter \"\"conversationtype\"\": conversationtype.\\nParameter \"\"input\"\": input.\\nParameter \"\"focusarea\"\": focusarea.\\nParameter \"\"previousresults\"\": previousresults.\\n// Translate the input into a language of your choice.\\nWriterPlugin.Translate\\nParameter \"\"input\"\": Text to translate.\\nParameter \"\"language\"\": Language to translate to.\\n// Summarize given text in two sentences or less.\\nWriterPlugin.TwoSentenceSummary\\nParameter \"\"input\"\": input.\\n// Generic function, unknown purpose.\\np_tSNsOTIMHWAhndYw.f_QOkQhOcvcIJkhbYg\\nParameter \"\"available_functions\"\": available_functions.\\nParameter \"\"goal\"\": goal.\\n// Given a request or command or goal generate a step by step plan to fulfill the request using functions. This ability is also known as decision making and function flow.\\nSequentialPlanner_Excluded.SequentialPlanner_Excluded\\nParameter \"\"input\"\": The question to answer.\\nParameter \"\"available_functions\"\": The list of the agent\\'s available_functions.\\n// Convert a string to lowercase.\\nTextPlugin.lowercase\\nNo parameters.\\n// Trim whitespace from the start and end of a string.\\nTextPlugin.trim\\nNo parameters.\\n// Trim whitespace from the end of a string.\\nTextPlugin.trim_end\\nNo parameters.\\n// Trim whitespace from the start of a string.\\nTextPlugin.trim_start\\nNo parameters.\\n// Convert a string to uppercase.\\nTextPlugin.uppercase\\nNo parameters.\\n// Adds value to a value.\\nmath.Add\\nParameter \"\"input\"\": The value to add.\\nParameter \"\"Amount\"\": Amount to add.\\n// Subtracts value to a value.\\nmath.Subtract\\nParameter \"\"input\"\": The value to subtract.\\nParameter \"\"Amount\"\": Amount to subtract.\\n// Read a file.\\nfileIO.readAsync\\nParameter \"\"input\"\": Path of the source file.\\n// Write a file.\\nfileIO.writeAsync\\nParameter \"\"content\"\": File content.\\nParameter \"\"path\"\": Destination path.\\n// Get the current date.\\ntime.date\\nNo parameters.\\n// Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era \\'Martedi\\' scorso -> dateMatchingLastDayName \\'Tuesday\\' => Tuesday,\\n        16 May, 2023.\\ntime.date_matching_last_day_name\\nNo parameters.\\n// Get the current day.\\ntime.day\\nNo parameters.\\n// Get the current day of the week.\\ntime.dayOfWeek\\nNo parameters.\\n// Get the date of offset from today by a provided number of days.\\ntime.days_ago\\nNo parameters.\\n// Get the current hour.\\ntime.hour\\nNo parameters.\\n// Get the current hour number.\\ntime.hourNumber\\nNo parameters.\\n// Get the current date in iso format.\\ntime.iso_date\\nNo parameters.\\n// Get the current minute.\\ntime.minute\\nNo parameters.\\n// Get the current month.\\ntime.month\\nNo parameters.\\n// Get the current month number.\\ntime.month_number\\nNo parameters.\\n// Get the current date and time in the local time zone.\\ntime.now\\nNo parameters.\\n// Get the seconds on the current minute.\\ntime.second\\nNo parameters.\\n// Get the current time in the local time zone.\\ntime.time\\nNo parameters.\\n// Get the current time zone name.\\ntime.timeZoneName\\nNo parameters.\\n// Get the current time zone offset.\\ntime.timeZoneOffset\\nNo parameters.\\n// Get the current date.\\ntime.today\\nNo parameters.\\n// Get the current date and time in UTC.\\ntime.utcNow\\nNo parameters.\\n// Get the current year.\\ntime.year\\nNo parameters.\\n// Convert a string to lowercase.\\ntext.lowercase\\nNo parameters.\\n// Trim whitespace from the start and end of a string.\\ntext.trim\\nNo parameters.\\n// Trim whitespace from the end of a string.\\ntext.trim_end\\nNo parameters.\\n// Trim whitespace from the start of a string.\\ntext.trim_start\\nNo parameters.\\n// Convert a string to uppercase.\\ntext.uppercase\\nNo parameters.\\n- End list of functions.\\nGoal: what is the sum of 22 and 22?\\n'}], extra_body=None), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x000001F7E96F18E0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'fileIO': KernelPlugin(name='fileIO', description=None, functions={'readAsync': KernelFunction(plugin_name='fileIO', description='Read a file', name='readAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.read of FileIOPlugin()>, parameters=[ParameterView(name='input', description='Path of the source file', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringOutTaskString: 10>, function=<bound method FileIOPlugin.read of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'writeAsync': KernelFunction(plugin_name='fileIO', description='Write a file', name='writeAsync', is_semantic=False, stream_function=<bound method FileIOPlugin.write of FileIOPlugin()>, parameters=[ParameterView(name='content', description='File content', default_value='', type_='string', required=False), ParameterView(name='path', description='Destination path', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InContextOutTask: 16>, function=<bound method FileIOPlugin.write of FileIOPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'time': KernelPlugin(name='time', description=None, functions={'date': KernelFunction(plugin_name='time', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='time', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='time', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='time', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='time', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='time', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='time', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='time', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='time', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='time', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='time', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='time', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='time', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='time', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='time', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='time', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='time', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='time', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='time', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'text': KernelPlugin(name='text', description=None, functions={'lowercase': KernelFunction(plugin_name='text', description='Convert a string to lowercase.', name='lowercase', is_semantic=False, stream_function=<bound method TextPlugin.lowercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.lowercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim': KernelFunction(plugin_name='text', description='Trim whitespace from the start and end of a string.', name='trim', is_semantic=False, stream_function=<bound method TextPlugin.trim of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_end': KernelFunction(plugin_name='text', description='Trim whitespace from the end of a string.', name='trim_end', is_semantic=False, stream_function=<bound method TextPlugin.trim_end of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_end of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'trim_start': KernelFunction(plugin_name='text', description='Trim whitespace from the start of a string.', name='trim_start', is_semantic=False, stream_function=<bound method TextPlugin.trim_start of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.trim_start of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'uppercase': KernelFunction(plugin_name='text', description='Convert a string to uppercase.', name='uppercase', is_semantic=False, stream_function=<bound method TextPlugin.uppercase of TextPlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TextPlugin.uppercase of TextPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'timee': KernelPlugin(name='timee', description=None, functions={'date': KernelFunction(plugin_name='timee', description='Get the current date.', name='date', is_semantic=False, stream_function=<bound method TimePlugin.date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'date_matching_last_day_name': KernelFunction(plugin_name='timee', description=\"Get the date of the last day matching the supplied week day name in English.\\n        Example: Che giorno era 'Martedi' scorso -> dateMatchingLastDayName 'Tuesday' => Tuesday,\\n        16 May, 2023\", name='date_matching_last_day_name', is_semantic=False, stream_function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.date_matching_last_day_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'day': KernelFunction(plugin_name='timee', description='Get the current day', name='day', is_semantic=False, stream_function=<bound method TimePlugin.day of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'dayOfWeek': KernelFunction(plugin_name='timee', description='Get the current day of the week', name='dayOfWeek', is_semantic=False, stream_function=<bound method TimePlugin.day_of_week of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.day_of_week of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'days_ago': KernelFunction(plugin_name='timee', description='Get the date of offset from today by a provided number of days', name='days_ago', is_semantic=False, stream_function=<bound method TimePlugin.days_ago of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.InStringOutString: 9>, function=<bound method TimePlugin.days_ago of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hour': KernelFunction(plugin_name='timee', description='Get the current hour', name='hour', is_semantic=False, stream_function=<bound method TimePlugin.hour of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'hourNumber': KernelFunction(plugin_name='timee', description='Get the current hour number', name='hourNumber', is_semantic=False, stream_function=<bound method TimePlugin.hour_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.hour_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'iso_date': KernelFunction(plugin_name='timee', description='Get the current date in iso format.', name='iso_date', is_semantic=False, stream_function=<bound method TimePlugin.iso_date of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.iso_date of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'minute': KernelFunction(plugin_name='timee', description='Get the current minute', name='minute', is_semantic=False, stream_function=<bound method TimePlugin.minute of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.minute of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month': KernelFunction(plugin_name='timee', description='Get the current month', name='month', is_semantic=False, stream_function=<bound method TimePlugin.month of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'month_number': KernelFunction(plugin_name='timee', description='Get the current month number', name='month_number', is_semantic=False, stream_function=<bound method TimePlugin.month_number of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.month_number of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'now': KernelFunction(plugin_name='timee', description='Get the current date and time in the local time zone', name='now', is_semantic=False, stream_function=<bound method TimePlugin.now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'second': KernelFunction(plugin_name='timee', description='Get the seconds on the current minute', name='second', is_semantic=False, stream_function=<bound method TimePlugin.second of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.second of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'time': KernelFunction(plugin_name='timee', description='Get the current time in the local time zone', name='time', is_semantic=False, stream_function=<bound method TimePlugin.time of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneName': KernelFunction(plugin_name='timee', description='Get the current time zone name', name='timeZoneName', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_name of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_name of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'timeZoneOffset': KernelFunction(plugin_name='timee', description='Get the current time zone offset', name='timeZoneOffset', is_semantic=False, stream_function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.time_zone_offset of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'today': KernelFunction(plugin_name='timee', description='Get the current date.', name='today', is_semantic=False, stream_function=<bound method TimePlugin.today of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.today of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'utcNow': KernelFunction(plugin_name='timee', description='Get the current date and time in UTC', name='utcNow', is_semantic=False, stream_function=<bound method TimePlugin.utc_now of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.utc_now of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'year': KernelFunction(plugin_name='timee', description='Get the current year', name='year', is_semantic=False, stream_function=<bound method TimePlugin.year of TimePlugin()>, parameters=[], delegate_type=<DelegateTypes.OutString: 2>, function=<bound method TimePlugin.year of TimePlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'mathh': KernelPlugin(name='mathh', description=None, functions={...})}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel.core_plugins.math_plugin import MathPlugin\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin \n",
    "\n",
    "kernel.import_plugin(TimePlugin(), \"timee\")\n",
    "kernel.import_plugin(MathPlugin(), \"mathh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = StepwisePlanner(kernel, StepwisePlannerConfig(max_iterations=10, min_iteration_time_ms=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"\"\"How many total championships combined do the top 5 teams in the NBA have ?\"\"\"\n",
    "\n",
    "plan = planner.create_plan(goal=ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await plan.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the steps that the AI took to get to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, step in enumerate(plan._steps):\n",
    "    print(\"Step:\", index)\n",
    "    print(\"Description:\", step.description)\n",
    "    print(\"Function:\", step.plugin_name + \".\" + step._function.name)\n",
    "    if len(step._outputs) > 0:\n",
    "        print(\"  Output:\\n\", str.replace(result[step._outputs[0]], \"\\n\", \"\\n  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
